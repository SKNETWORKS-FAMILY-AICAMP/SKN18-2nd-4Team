# ğŸ“Š E-Commerce ê³ ê° ì´íƒˆ ì˜ˆì¸¡ í”„ë¡œì íŠ¸
# 02. ìƒì„¸ í”¼ì³ ë¶„ì„
# 
# ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” E-Commerce ë°ì´í„°ì…‹ì˜ í”¼ì³ë“¤ì„ ìƒì„¸íˆ ë¶„ì„í•˜ê³  ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

# ============================================================================
# ì…€ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!")

# ============================================================================
# ì…€ 2: ë°ì´í„° ë¡œë”©
# ============================================================================

# ë°ì´í„° ë¡œë“œ
df = pd.read_excel("data/raw/E Commerce Dataset.xlsx", sheet_name='E Comm')
print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")

# ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„±
df_analysis = df.copy()

# ============================================================================
# ì…€ 3: ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
# ============================================================================

print("ğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
print(f"- ì´ ê³ ê° ìˆ˜: {len(df_analysis):,}ëª…")
print(f"- ì „ì²´ ì´íƒˆë¥ : {df_analysis['Churn'].mean()*100:.1f}%")
print(f"- ê²°ì¸¡ê°’ ë¹„ìœ¨: {(df_analysis.isnull().sum().sum()/len(df_analysis)/len(df_analysis.columns)*100):.1f}%")

# ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜
numeric_cols = df_analysis.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df_analysis.select_dtypes(include=['object', 'category']).columns.tolist()

print(f"\nğŸ“‹ ì»¬ëŸ¼ ë¶„ë¥˜:")
print(f"- ìˆ˜ì¹˜í˜•: {len(numeric_cols)}ê°œ")
print(f"- ë²”ì£¼í˜•: {len(categorical_cols)}ê°œ")

# ============================================================================
# ì…€ 4: ê²°ì¸¡ê°’ ì²˜ë¦¬
# ============================================================================

print("ğŸ” ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „:")
missing_before = df_analysis.isnull().sum()
print(missing_before[missing_before > 0])

# ê²°ì¸¡ê°’ ì²˜ë¦¬
# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
numeric_missing_cols = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 
                       'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder']

for col in numeric_missing_cols:
    if col in df_analysis.columns:
        median_val = df_analysis[col].median()
        df_analysis[col].fillna(median_val, inplace=True)
        print(f"âœ… {col}: ì¤‘ì•™ê°’ {median_val:.2f}ìœ¼ë¡œ ê²°ì¸¡ê°’ ëŒ€ì²´")

print("\nğŸ” ê²°ì¸¡ê°’ ì²˜ë¦¬ í›„:")
missing_after = df_analysis.isnull().sum()
print(missing_after[missing_after > 0])

# ============================================================================
# ì…€ 5: ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
# ============================================================================

# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
label_encoders = {}
categorical_features = ['Gender', 'MaritalStatus', 'CityTier', 'PreferredPaymentMode', 'PreferedOrderCat']

for col in categorical_features:
    if col in df_analysis.columns:
        le = LabelEncoder()
        df_analysis[f'{col}_encoded'] = le.fit_transform(df_analysis[col])
        label_encoders[col] = le
        print(f"âœ… {col} ì¸ì½”ë”© ì™„ë£Œ: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# ============================================================================
# ì…€ 6: í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ìƒˆë¡œìš´ í”¼ì³ ìƒì„±
# ============================================================================

print("ğŸ”§ ìƒˆë¡œìš´ í”¼ì³ ìƒì„±:")

# 1. ê³ ê° í™œë™ì„± ì ìˆ˜ (Activity Score)
df_analysis['ActivityScore'] = (
    df_analysis['OrderCount'] * 0.4 + 
    df_analysis['HourSpendOnApp'] * 0.3 + 
    df_analysis['CouponUsed'] * 0.3
)
print("âœ… ActivityScore ìƒì„± ì™„ë£Œ")

# 2. ê³ ê° ë§Œì¡±ë„ ë ˆë²¨
df_analysis['SatisfactionLevel'] = pd.cut(
    df_analysis['SatisfactionScore'], 
    bins=[0, 2, 3, 5], 
    labels=['Low', 'Medium', 'High']
)
print("âœ… SatisfactionLevel ìƒì„± ì™„ë£Œ")

# 3. ê³ ê° ìœ ì§€ ê¸°ê°„ ê·¸ë£¹
df_analysis['TenureGroup'] = pd.cut(
    df_analysis['Tenure'], 
    bins=[0, 6, 12, 24, 100], 
    labels=['New', 'Short', 'Medium', 'Long']
)
print("âœ… TenureGroup ìƒì„± ì™„ë£Œ")

# 4. ì£¼ë¬¸ ë¹ˆë„ (Order Frequency)
df_analysis['OrderFrequency'] = df_analysis['OrderCount'] / (df_analysis['Tenure'] + 1)
print("âœ… OrderFrequency ìƒì„± ì™„ë£Œ")

# 5. ê³ ê° ê°€ì¹˜ ì ìˆ˜ (Customer Value Score)
df_analysis['CustomerValueScore'] = (
    df_analysis['CashbackAmount'] * 0.5 + 
    df_analysis['OrderCount'] * 0.3 + 
    df_analysis['Tenure'] * 0.2
)
print("âœ… CustomerValueScore ìƒì„± ì™„ë£Œ")

# ============================================================================
# ì…€ 7: í”¼ì³ ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================================================

# ìˆ˜ì¹˜í˜• í”¼ì³ë“¤ ì„ íƒ (ìƒˆë¡œ ìƒì„±ëœ í”¼ì³ í¬í•¨)
numeric_features = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered',
                   'SatisfactionScore', 'NumberOfAddress', 'OrderAmountHikeFromlastYear',
                   'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount',
                   'ActivityScore', 'OrderFrequency', 'CustomerValueScore', 'Churn']

# ìƒê´€ê´€ê³„ ê³„ì‚°
correlation_matrix = df_analysis[numeric_features].corr()

# Churnê³¼ì˜ ìƒê´€ê´€ê³„
churn_correlations = correlation_matrix['Churn'].sort_values(ascending=False)
print("ğŸ¯ Churnê³¼ì˜ ìƒê´€ê´€ê³„ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
for feature, corr in churn_correlations.items():
    if feature != 'Churn':
        print(f"  {feature}: {corr:.3f}")

# ============================================================================
# ì…€ 8: ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™”
# ============================================================================

plt.figure(figsize=(16, 12))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": .8}, fmt='.2f')
plt.title('í”¼ì³ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# ============================================================================
# ì…€ 9: Churnê³¼ì˜ ìƒê´€ê´€ê³„ ì‹œê°í™”
# ============================================================================

# Churnê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì³ë“¤ ì‹œê°í™”
high_corr_features = ['Tenure', 'DaySinceLastOrder', 'CashbackAmount', 'OrderCount', 'SatisfactionScore']

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, feature in enumerate(high_corr_features):
    if i < len(axes):
        # ë°•ìŠ¤í”Œë¡¯
        df_analysis.boxplot(column=feature, by='Churn', ax=axes[i])
        axes[i].set_title(f'{feature} vs Churn')
        axes[i].set_xlabel('Churn')
        axes[i].set_ylabel(feature)
        axes[i].set_xticklabels(['ìœ ì§€', 'ì´íƒˆ'])

# ë§ˆì§€ë§‰ subplot ì œê±°
if len(high_corr_features) < 6:
    fig.delaxes(axes[5])

plt.tight_layout()
plt.show()

# ============================================================================
# ì…€ 10: ë²”ì£¼í˜• ë³€ìˆ˜ì™€ Churnì˜ ê´€ê³„ ë¶„ì„
# ============================================================================

# ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ì´íƒˆë¥  ë¶„ì„
categorical_analysis = ['Gender', 'MaritalStatus', 'CityTier', 'PreferredPaymentMode', 'PreferedOrderCat']

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, feature in enumerate(categorical_analysis):
    if i < len(axes):
        # ì´íƒˆë¥  ê³„ì‚°
        churn_rate = df_analysis.groupby(feature)['Churn'].mean() * 100
        
        # ë§‰ëŒ€ ì°¨íŠ¸
        bars = axes[i].bar(range(len(churn_rate)), churn_rate.values, color='lightcoral')
        axes[i].set_title(f'{feature}ë³„ ì´íƒˆë¥ ')
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel('ì´íƒˆë¥  (%)')
        axes[i].set_xticks(range(len(churn_rate)))
        axes[i].set_xticklabels(churn_rate.index, rotation=45, ha='right')
        
        # ê°’ í‘œì‹œ
        for bar, rate in zip(bars, churn_rate.values):
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{rate:.1f}%', ha='center', va='bottom')

# ë§ˆì§€ë§‰ subplot ì œê±°
if len(categorical_analysis) < 6:
    fig.delaxes(axes[5])

plt.tight_layout()
plt.show()

# ============================================================================
# ì…€ 11: ìƒˆë¡œìš´ í”¼ì³ë“¤ì˜ ë¶„í¬ ë¶„ì„
# ============================================================================

# ìƒˆë¡œ ìƒì„±ëœ í”¼ì³ë“¤ì˜ ë¶„í¬
new_features = ['ActivityScore', 'OrderFrequency', 'CustomerValueScore']

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, feature in enumerate(new_features):
    # Churnë³„ ë¶„í¬
    axes[i].hist(df_analysis[df_analysis['Churn']==0][feature], alpha=0.7, 
                label='ìœ ì§€', bins=20, color='lightblue')
    axes[i].hist(df_analysis[df_analysis['Churn']==1][feature], alpha=0.7, 
                label='ì´íƒˆ', bins=20, color='lightcoral')
    axes[i].set_title(f'{feature} ë¶„í¬')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('ë¹ˆë„')
    axes[i].legend()

plt.tight_layout()
plt.show()

# ============================================================================
# ì…€ 12: í”¼ì³ ì¤‘ìš”ë„ ë¶„ì„ (Random Forest)
# ============================================================================

# ëª¨ë¸ë§ìš© í”¼ì³ ì„ íƒ
modeling_features = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered',
                    'SatisfactionScore', 'NumberOfAddress', 'OrderAmountHikeFromlastYear',
                    'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount',
                    'ActivityScore', 'OrderFrequency', 'CustomerValueScore']

# ì¸ì½”ë”©ëœ ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
for col in categorical_features:
    if f'{col}_encoded' in df_analysis.columns:
        modeling_features.append(f'{col}_encoded')

# ë°ì´í„° ì¤€ë¹„
X = df_analysis[modeling_features].fillna(0)
y = df_analysis['Churn']

# Random Forestë¡œ í”¼ì³ ì¤‘ìš”ë„ ê³„ì‚°
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# í”¼ì³ ì¤‘ìš”ë„
feature_importance = pd.DataFrame({
    'feature': modeling_features,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("ğŸ¯ Random Forest í”¼ì³ ì¤‘ìš”ë„:")
for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
    print(f"{i:2d}. {row['feature']}: {row['importance']:.4f}")

# ============================================================================
# ì…€ 13: í”¼ì³ ì¤‘ìš”ë„ ì‹œê°í™”
# ============================================================================

plt.figure(figsize=(12, 8))
top_features = feature_importance.head(15)
bars = plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('í”¼ì³ ì¤‘ìš”ë„')
plt.title('Random Forest í”¼ì³ ì¤‘ìš”ë„ (ìƒìœ„ 15ê°œ)')
plt.gca().invert_yaxis()

# ê°’ í‘œì‹œ
for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):
    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
             f'{importance:.3f}', ha='left', va='center')

plt.tight_layout()
plt.show()

# ============================================================================
# ì…€ 14: í†µê³„ì  ìœ ì˜ì„± ê²€ì •
# ============================================================================

print("ğŸ“Š í†µê³„ì  ìœ ì˜ì„± ê²€ì • (Churn vs ì£¼ìš” í”¼ì³):")
print("=" * 60)

# ì£¼ìš” ìˆ˜ì¹˜í˜• í”¼ì³ë“¤ì— ëŒ€í•œ t-test
numeric_test_features = ['Tenure', 'OrderCount', 'SatisfactionScore', 'CashbackAmount', 'ActivityScore']

for feature in numeric_test_features:
    # ìœ ì§€ ê³ ê°ê³¼ ì´íƒˆ ê³ ê° ë¶„ë¦¬
    loyal = df_analysis[df_analysis['Churn'] == 0][feature]
    churned = df_analysis[df_analysis['Churn'] == 1][feature]
    
    # t-test ìˆ˜í–‰
    t_stat, p_value = stats.ttest_ind(loyal, churned)
    
    print(f"\nğŸ“Œ {feature}:")
    print(f"  - ìœ ì§€ ê³ ê° í‰ê· : {loyal.mean():.2f}")
    print(f"  - ì´íƒˆ ê³ ê° í‰ê· : {churned.mean():.2f}")
    print(f"  - t-statistic: {t_stat:.4f}")
    print(f"  - p-value: {p_value:.4f}")
    print(f"  - ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")

# ============================================================================
# ì…€ 15: ë²”ì£¼í˜• ë³€ìˆ˜ ì¹´ì´ì œê³± ê²€ì •
# ============================================================================

print("\nğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ì¹´ì´ì œê³± ê²€ì •:")
print("=" * 60)

for feature in categorical_features:
    # êµì°¨í‘œ ìƒì„±
    contingency_table = pd.crosstab(df_analysis[feature], df_analysis['Churn'])
    
    # ì¹´ì´ì œê³± ê²€ì •
    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
    
    print(f"\nğŸ“Œ {feature}:")
    print(f"  - ì¹´ì´ì œê³± í†µê³„ëŸ‰: {chi2:.4f}")
    print(f"  - p-value: {p_value:.4f}")
    print(f"  - ììœ ë„: {dof}")
    print(f"  - ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")

# ============================================================================
# ì…€ 16: ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
# ============================================================================

print("ğŸ¯ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„:")
print("=" * 50)

# 1. ë§Œì¡±ë„ë³„ ì„¸ê·¸ë¨¼íŠ¸
print("\n1. ë§Œì¡±ë„ë³„ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸:")
satisfaction_segments = df_analysis.groupby('SatisfactionLevel')['Churn'].agg(['count', 'sum', 'mean'])
for segment in satisfaction_segments.index:
    count = satisfaction_segments.loc[segment, 'count']
    churn_count = satisfaction_segments.loc[segment, 'sum']
    churn_rate = satisfaction_segments.loc[segment, 'mean'] * 100
    print(f"  {segment}: {count}ëª…, ì´íƒˆë¥  {churn_rate:.1f}% ({churn_count}ëª…)")

# 2. ê³ ê° ìœ ì§€ ê¸°ê°„ë³„ ì„¸ê·¸ë¨¼íŠ¸
print("\n2. ê³ ê° ìœ ì§€ ê¸°ê°„ë³„ ì„¸ê·¸ë¨¼íŠ¸:")
tenure_segments = df_analysis.groupby('TenureGroup')['Churn'].agg(['count', 'sum', 'mean'])
for segment in tenure_segments.index:
    count = tenure_segments.loc[segment, 'count']
    churn_count = tenure_segments.loc[segment, 'sum']
    churn_rate = tenure_segments.loc[segment, 'mean'] * 100
    print(f"  {segment}: {count}ëª…, ì´íƒˆë¥  {churn_rate:.1f}% ({churn_count}ëª…)")

# 3. í™œë™ì„± ì ìˆ˜ë³„ ì„¸ê·¸ë¨¼íŠ¸
print("\n3. í™œë™ì„± ì ìˆ˜ë³„ ì„¸ê·¸ë¨¼íŠ¸:")
df_analysis['ActivityLevel'] = pd.qcut(df_analysis['ActivityScore'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])
activity_segments = df_analysis.groupby('ActivityLevel')['Churn'].agg(['count', 'sum', 'mean'])
for segment in activity_segments.index:
    count = activity_segments.loc[segment, 'count']
    churn_count = activity_segments.loc[segment, 'sum']
    churn_rate = activity_segments.loc[segment, 'mean'] * 100
    print(f"  {segment}: {count}ëª…, ì´íƒˆë¥  {churn_rate:.1f}% ({churn_count}ëª…)")

# ============================================================================
# ì…€ 17: ê³ ìœ„í—˜ ê³ ê° í”„ë¡œí•„ ë¶„ì„
# ============================================================================

print("âš ï¸ ê³ ìœ„í—˜ ê³ ê° í”„ë¡œí•„ ë¶„ì„:")
print("=" * 50)

# ê³ ìœ„í—˜ ê³ ê° ì •ì˜ (ì´íƒˆë¥ ì´ ë†’ì€ ì¡°ê±´ë“¤)
high_risk_conditions = [
    (df_analysis['SatisfactionScore'] <= 2) & (df_analysis['OrderCount'] <= 2),
    (df_analysis['Tenure'] <= 6) & (df_analysis['Complain'] == 1),
    (df_analysis['MaritalStatus'] == 'Single') & (df_analysis['CityTier'] == 3),
    (df_analysis['DaySinceLastOrder'] >= 10) & (df_analysis['OrderCount'] <= 1)
]

risk_profiles = ['ë‚®ì€ ë§Œì¡±ë„ + ë‚®ì€ ì£¼ë¬¸', 'ì‹ ê·œ ê³ ê° + ë¶ˆë§Œ ì œê¸°', 
                'Single + Tier 3', 'ì¥ê¸° ë¯¸ì£¼ë¬¸ + ë‚®ì€ ì£¼ë¬¸']

for i, (condition, profile) in enumerate(zip(high_risk_conditions, risk_profiles), 1):
    high_risk_customers = df_analysis[condition]
    if len(high_risk_customers) > 0:
        churn_rate = high_risk_customers['Churn'].mean() * 100
        print(f"\n{i}. {profile}:")
        print(f"   - ê³ ê° ìˆ˜: {len(high_risk_customers):,}ëª…")
        print(f"   - ì´íƒˆë¥ : {churn_rate:.1f}%")
        print(f"   - ì „ì²´ ëŒ€ë¹„ ì´íƒˆë¥ : {churn_rate/df_analysis['Churn'].mean()*100:.1f}ë°°")

# ============================================================================
# ì…€ 18: í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸
# ============================================================================

print("ğŸ’¡ í”¼ì³ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­:")
print("=" * 60)

print("\nğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­:")
print("1. ê³ ê° ìœ ì§€ ê¸°ê°„(Tenure)ì´ ì´íƒˆê³¼ ê°€ì¥ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„")
print("2. ë§ˆì§€ë§‰ ì£¼ë¬¸ í›„ ê²½ê³¼ì¼(DaySinceLastOrder)ì´ ì´íƒˆê³¼ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„")
print("3. ìºì‹œë°± ê¸ˆì•¡ì´ ë†’ì„ìˆ˜ë¡ ì´íƒˆë¥ ì´ ë‚®ìŒ")
print("4. Single ê³ ê°ê³¼ Tier 3 ë„ì‹œ ê³ ê°ì˜ ì´íƒˆë¥ ì´ ë†’ìŒ")
print("5. ë§Œì¡±ë„ê°€ ë‚®ê³  ì£¼ë¬¸ íšŸìˆ˜ê°€ ì ì€ ê³ ê°ì´ ê³ ìœ„í—˜êµ°")

print("\nğŸ”§ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:")
print("1. ActivityScore: ê³ ê° í™œë™ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ìƒˆë¡œìš´ í”¼ì³")
print("2. OrderFrequency: ì£¼ë¬¸ ë¹ˆë„ë¥¼ ì¸¡ì •í•˜ëŠ” í”¼ì³")
print("3. CustomerValueScore: ê³ ê° ê°€ì¹˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í”¼ì³")
print("4. SatisfactionLevel, TenureGroup: ë²”ì£¼í˜• í”¼ì³ë¡œ ë³€í™˜")

print("\nğŸ¯ ëª¨ë¸ë§ ê¶Œì¥ì‚¬í•­:")
print("1. ìƒìœ„ 10ê°œ í”¼ì³ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©")
print("2. ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì¸ì½”ë”© í›„ ì‚¬ìš©")
print("3. ê²°ì¸¡ê°’ì€ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´")
print("4. í”¼ì³ ìŠ¤ì¼€ì¼ë§ ì ìš© ê³ ë ¤")
print("5. ê³ ìœ„í—˜ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ê°œë³„ ëª¨ë¸ ê³ ë ¤")

# ============================================================================
# ë§ˆí¬ë‹¤ìš´ ì…€ ë‚´ìš© (ë³µì‚¬í•´ì„œ ë…¸íŠ¸ë¶ì— ë¶™ì—¬ë„£ê¸°)
# ============================================================================

"""
# ğŸ“Š E-Commerce ê³ ê° ì´íƒˆ ì˜ˆì¸¡ í”„ë¡œì íŠ¸
## 02. ìƒì„¸ í”¼ì³ ë¶„ì„

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” E-Commerce ë°ì´í„°ì…‹ì˜ í”¼ì³ë“¤ì„ ìƒì„¸íˆ ë¶„ì„í•˜ê³  ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ğŸ“‹ ëª©ì°¨
1. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸](#1-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„í¬íŠ¸)
2. [ë°ì´í„° ë¡œë”©](#2-ë°ì´í„°-ë¡œë”©)
3. [ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸](#3-ë°ì´í„°-ê¸°ë³¸-ì •ë³´-í™•ì¸)
4. [ê²°ì¸¡ê°’ ì²˜ë¦¬](#4-ê²°ì¸¡ê°’-ì²˜ë¦¬)
5. [ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©](#5-ë²”ì£¼í˜•-ë³€ìˆ˜-ì¸ì½”ë”©)
6. [í”¼ì³ ì—”ì§€ë‹ˆì–´ë§](#6-í”¼ì³-ì—”ì§€ë‹ˆì–´ë§)
7. [í”¼ì³ ìƒê´€ê´€ê³„ ë¶„ì„](#7-í”¼ì³-ìƒê´€ê´€ê³„-ë¶„ì„)
8. [ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ](#8-ìƒê´€ê´€ê³„-íˆíŠ¸ë§µ)
9. [Churnê³¼ì˜ ê´€ê³„ ì‹œê°í™”](#9-churnê³¼ì˜-ê´€ê³„-ì‹œê°í™”)
10. [ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„](#10-ë²”ì£¼í˜•-ë³€ìˆ˜-ë¶„ì„)
11. [ìƒˆë¡œìš´ í”¼ì³ ë¶„í¬](#11-ìƒˆë¡œìš´-í”¼ì³-ë¶„í¬)
12. [í”¼ì³ ì¤‘ìš”ë„ ë¶„ì„](#12-í”¼ì³-ì¤‘ìš”ë„-ë¶„ì„)
13. [í”¼ì³ ì¤‘ìš”ë„ ì‹œê°í™”](#13-í”¼ì³-ì¤‘ìš”ë„-ì‹œê°í™”)
14. [í†µê³„ì  ìœ ì˜ì„± ê²€ì •](#14-í†µê³„ì -ìœ ì˜ì„±-ê²€ì •)
15. [ì¹´ì´ì œê³± ê²€ì •](#15-ì¹´ì´ì œê³±-ê²€ì •)
16. [ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„](#16-ê³ ê°-ì„¸ê·¸ë¨¼íŠ¸-ë¶„ì„)
17. [ê³ ìœ„í—˜ ê³ ê° í”„ë¡œí•„](#17-ê³ ìœ„í—˜-ê³ ê°-í”„ë¡œí•„)
18. [ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­](#18-ì¸ì‚¬ì´íŠ¸-ë°-ê¶Œì¥ì‚¬í•­)

## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

## 2. ë°ì´í„° ë¡œë”©

## 3. ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸

## 4. ê²°ì¸¡ê°’ ì²˜ë¦¬

## 5. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©

## 6. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§

## 7. í”¼ì³ ìƒê´€ê´€ê³„ ë¶„ì„

## 8. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ

## 9. Churnê³¼ì˜ ê´€ê³„ ì‹œê°í™”

## 10. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„

## 11. ìƒˆë¡œìš´ í”¼ì³ ë¶„í¬

## 12. í”¼ì³ ì¤‘ìš”ë„ ë¶„ì„

## 13. í”¼ì³ ì¤‘ìš”ë„ ì‹œê°í™”

## 14. í†µê³„ì  ìœ ì˜ì„± ê²€ì •

## 15. ì¹´ì´ì œê³± ê²€ì •

## 16. ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„

## 17. ê³ ìœ„í—˜ ê³ ê° í”„ë¡œí•„

## 18. ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­

## ğŸ“ ìš”ì•½

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” E-Commerce ë°ì´í„°ì…‹ì˜ í”¼ì³ë“¤ì„ ìƒì„¸íˆ ë¶„ì„í•˜ê³  ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ë°œê²¬ì‚¬í•­:
- **ê³ ê° ìœ ì§€ ê¸°ê°„**: ì´íƒˆê³¼ ê°€ì¥ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„
- **ë§ˆì§€ë§‰ ì£¼ë¬¸ í›„ ê²½ê³¼ì¼**: ì´íƒˆê³¼ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„  
- **ìºì‹œë°± ê¸ˆì•¡**: ë†’ì„ìˆ˜ë¡ ì´íƒˆë¥  ê°ì†Œ
- **ê³ ìœ„í—˜ ê³ ê°**: Single + Tier 3, ë‚®ì€ ë§Œì¡±ë„ + ë‚®ì€ ì£¼ë¬¸

### í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:
- ActivityScore: ê³ ê° í™œë™ì„± ì¢…í•© ì§€í‘œ
- OrderFrequency: ì£¼ë¬¸ ë¹ˆë„ ì§€í‘œ
- CustomerValueScore: ê³ ê° ê°€ì¹˜ ì¢…í•© ì§€í‘œ

### ë‹¤ìŒ ë‹¨ê³„:
1. ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬
2. ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‹¤í—˜
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° í•´ì„
"""
