#!/usr/bin/env python3
"""
ëª¨ë¸ ì„±ëŠ¥ ì ìˆ˜ë¥¼ ìƒì„¸íˆ ì €ì¥í•˜ê³  ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import joblib
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def save_model_performance(model_results=None):
    """ëª¨ë¸ ì„±ëŠ¥ ì ìˆ˜ë¥¼ ìƒì„¸íˆ ì €ì¥í•˜ê³  ì¶œë ¥"""
    try:
        # ê²½ë¡œ ì„¤ì •
        outputs_dir = Path("outputs")
        outputs_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ê²°ê³¼ í™•ì¸
        if model_results is None:
            model_results_path = outputs_dir / "model_results.pkl"
            if not model_results_path.exists():
                logger.error("model_results.pkl íŒŒì¼ì´ ì—†ê³  model_resultsë„ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            # íŒŒì¼ì—ì„œ ë¡œë“œ
            model_results = joblib.load(model_results_path)
        
        # model_details ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ model_comparison ì‚¬ìš©
        if 'model_details' in model_results:
            model_data = model_results['model_details']
            use_details = True
        elif 'model_comparison' in model_results:
            model_data = model_results['model_comparison']
            use_details = False
        else:
            logger.error("ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìƒì„¸ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        detailed_performance = []
        
        for model_name, metrics in model_data.items():
            if use_details:
                # model_details ì‚¬ìš© (ì‹¤ì œ ì„±ëŠ¥ ì§€í‘œ í¬í•¨)
                detailed_performance.append({
                    'Model': model_name,
                    'Accuracy': round(metrics.get('accuracy', 0), 4),
                    'Precision': round(metrics.get('precision', 0), 4),
                    'Recall': round(metrics.get('recall', 0), 4),
                    'F1_Score': round(metrics.get('f1', 0), 4),
                    'AUC': round(metrics.get('auc', 0), 4),
                    'Composite_Score': round(metrics.get('composite_score', 0), 4)
                })
            else:
                # metricsê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
                if isinstance(metrics, (int, float)):
                    # ë‹¨ìˆœ ì ìˆ˜ì¸ ê²½ìš° Composite_Scoreë¡œ ì²˜ë¦¬
                    detailed_performance.append({
                        'Model': model_name,
                        'Accuracy': 0,
                        'Precision': 0,
                        'Recall': 0,
                        'F1_Score': 0,
                        'AUC': 0,
                        'Composite_Score': round(metrics, 4)
                    })
                else:
                    # dictì¸ ê²½ìš° ì •ìƒ ì²˜ë¦¬
                    detailed_performance.append({
                        'Model': model_name,
                        'Accuracy': round(metrics.get('Accuracy', 0), 4),
                        'Precision': round(metrics.get('Precision', 0), 4),
                        'Recall': round(metrics.get('Recall', 0), 4),
                        'F1_Score': round(metrics.get('F1_Score', 0), 4),
                        'AUC': round(metrics.get('AUC', 0), 4),
                        'Composite_Score': round(metrics.get('Composite_Score', 0), 4)
                    })
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬
        df_performance = pd.DataFrame(detailed_performance)
        df_performance = df_performance.sort_values('Composite_Score', ascending=False)
        df_performance['Rank'] = range(1, len(df_performance) + 1)
        
        # ìˆœìœ„ë³„ ì´ëª¨ì§€ ì¶”ê°€
        rank_emoji = {1: 'ğŸ¥‡', 2: 'ğŸ¥ˆ', 3: 'ğŸ¥‰'}
        df_performance['Medal'] = df_performance['Rank'].map(lambda x: rank_emoji.get(x, ''))
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        df_performance = df_performance[['Rank', 'Medal', 'Model', 'Composite_Score', 
                                       'Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC']]
        
        # CSV ì €ì¥
        performance_csv_path = outputs_dir / "detailed_model_performance.csv"
        df_performance.to_csv(performance_csv_path, index=False, encoding='utf-8-sig')
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š **ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ê²°ê³¼**")
        print("="*80)
        
        for idx, row in df_performance.iterrows():
            print(f"\n{row['Medal']} **{row['Rank']}ìœ„: {row['Model']}**")
            print(f"   â€¢ ì¢…í•©ì ìˆ˜: {row['Composite_Score']:.4f}")
            print(f"   â€¢ ì •í™•ë„  : {row['Accuracy']:.4f}")
            print(f"   â€¢ ì •ë°€ë„  : {row['Precision']:.4f}")
            print(f"   â€¢ ì¬í˜„ìœ¨  : {row['Recall']:.4f}")
            print(f"   â€¢ F1ì ìˆ˜  : {row['F1_Score']:.4f}")
            print(f"   â€¢ AUC     : {row['AUC']:.4f}")
        
        print("\n" + "="*80)
        print(f"âœ… ìƒì„¸ ì„±ëŠ¥ ê²°ê³¼ê°€ {performance_csv_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì±„íƒ ëª¨ë¸ ì •ë³´ ì €ì¥
        best_model_info = {
            'Best_Model': df_performance.iloc[0]['Model'],
            'Best_Score': df_performance.iloc[0]['Composite_Score'],
            'Selection_Criteria': 'Composite Score (Weighted Average)',
            'Weight_Formula': 'Precision(0.4) + F1(0.3) + Accuracy(0.2) + Recall(0.1)'
        }
        
        # ì±„íƒ ëª¨ë¸ ì •ë³´ CSV ì €ì¥
        adoption_csv_path = outputs_dir / "model_adoption_info.csv"
        pd.DataFrame([best_model_info]).to_csv(adoption_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ëª¨ë¸ ì±„íƒ ì •ë³´ê°€ {adoption_csv_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"âœ… ëª¨ë¸ ì„±ëŠ¥ ì €ì¥ ì™„ë£Œ: {len(df_performance)}ê°œ ëª¨ë¸")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì„±ëŠ¥ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    save_model_performance()

if __name__ == "__main__":
    main()