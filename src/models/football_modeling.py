"""
Football Transfer Prediction - Modeling Pipeline
ì‹¤ì œ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ë§ í´ë˜ìŠ¤
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from typing import Dict, Any, List, Tuple
import logging

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€
from src.features.feature_engineering import FootballFeatureEngineer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, 
                           precision_score, recall_score, f1_score, roc_auc_score, 
                           roc_curve, precision_recall_curve, auc)
from sklearn.utils.class_weight import compute_class_weight

# Optional libraries
try:
    import xgboost as xgb
    from xgboost import XGBClassifier
    _has_xgb = True
except ImportError:
    _has_xgb = False

try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
    _has_lgbm = True
except ImportError:
    _has_lgbm = False

try:
    import shap
    _has_shap = True
except ImportError:
    _has_shap = False

logger = logging.getLogger(__name__)

class CustomLabelEncoder(BaseEstimator, TransformerMixin):
    """íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ ì¸ì½”ë”"""
    
    def __init__(self):
        self.label_encoders = {}
        self.is_fitted = False
    
    def fit(self, X, y=None):
        self.label_encoders = {}
        if hasattr(X, 'iloc'):
            for i in range(X.shape[1]):
                le = LabelEncoder()
                le.fit(X.iloc[:, i].astype(str))
                self.label_encoders[i] = le
        else:
            for i in range(X.shape[1]):
                le = LabelEncoder()
                le.fit(X[:, i].astype(str))
                self.label_encoders[i] = le
        self.is_fitted = True
        return self
    
    def transform(self, X):
        if not self.is_fitted:
            raise ValueError("LabelEncoderê°€ fitë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        X_encoded = np.zeros_like(X, dtype=float)
        
        if hasattr(X, 'iloc'):
            for i in range(X.shape[1]):
                try:
                    X_encoded[:, i] = self.label_encoders[i].transform(X.iloc[:, i].astype(str))
                except ValueError:
                    # ìƒˆë¡œìš´ ë¼ë²¨ì´ ìˆëŠ” ê²½ìš° -1ë¡œ ë§¤í•‘
                    X_encoded[:, i] = np.array([
                        self.label_encoders[i].transform([x])[0] if x in self.label_encoders[i].classes_ else -1
                        for x in X.iloc[:, i].astype(str)
                    ])
        else:
            for i in range(X.shape[1]):
                try:
                    X_encoded[:, i] = self.label_encoders[i].transform(X[:, i].astype(str))
                except ValueError:
                    X_encoded[:, i] = np.array([
                        self.label_encoders[i].transform([x])[0] if x in self.label_encoders[i].classes_ else -1
                        for x in X[:, i].astype(str)
                    ])
        return X_encoded.astype(float)

class FootballModelTrainer:
    """Football Transfer Prediction ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, df_model: pd.DataFrame, config):
        self.df_model = df_model
        self.config = config
        self.target_col = config.target_column
        self.ordinal_features = config.features_ordinal
        self.nominal_features = config.features_nominal
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.model_results = {}
        self.best_model = None
        self.preprocessor = None
        
    def run_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
        feature_engineer = FootballFeatureEngineer()
        self.df_model, self.preprocessor, self.feature_types = feature_engineer.fit_transform(self.df_model)
        logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {self.df_model.shape}")
        
        # 2. ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test, X_2324 = self._split_data()
        
        # 3. ëª¨ë¸ ì •ì˜
        models = self._define_models()
        
        # 4. ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        model_scores = self._train_and_evaluate_models(models, X_train, y_train, X_test, y_test)
        
        # 5. ìµœì  ëª¨ë¸ ì„ íƒ
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = models[best_model_name]
        
        # 6. ìµœì¢… í‰ê°€
        final_results = self._final_evaluation(X_test, y_test)
        
        # 7. SHAP ë¶„ì„
        shap_results = self._shap_analysis(X_test, y_test)
        
        # 8. ê²°ê³¼ ì €ì¥
        self.model_results = {
            'model_scores': model_scores,
            'model_comparison': model_scores,  # Plotterì—ì„œ ì‚¬ìš©
            'best_model_name': best_model_name,
            'best_model': self.best_model,
            'preprocessor': self.preprocessor,
            'final_results': final_results,
            'shap_results': shap_results,
            'X_test': X_test,
            'y_test': y_test,
            'X_2324': X_2324
        }
        
        # 9. ì‹œê°í™” (SHAP, í”¼ì²˜ ì¤‘ìš”ë„)
        try:
            from src.visualization.plotter import ModelVisualizer
            visualizer = ModelVisualizer(self.model_results, self.output_dir)
            
            # SHAP ë¶„ì„ í”Œë¡¯
            visualizer.plot_shap_analysis()
            
            # í”¼ì²˜ ì¤‘ìš”ë„ í”Œë¡¯  
            visualizer.plot_feature_importance()
            
            logger.info("âœ… ì‹œê°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì‹œê°í™” ì˜¤ë¥˜: {e}")
        
        logger.info("âœ… ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return self.model_results
    
    def _get_numeric_features(self) -> List[str]:
        """ìˆ˜ì¹˜í˜• í”¼ì²˜ ì¶”ì¶œ (ID ë³€ìˆ˜ ì œì™¸)"""
        all_features = set(self.df_model.columns) - {self.target_col}
        categorical_features = set(self.ordinal_features + self.nominal_features)
        # ì œì™¸í•  ID ë³€ìˆ˜ë“¤
        exclude_cols = {'player_id', 'club_id', 'season'}
        
        return [col for col in all_features if col not in categorical_features and 
                col not in exclude_cols and
                pd.api.types.is_numeric_dtype(self.df_model[col])]
    
    def _split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame]:
        """ë°ì´í„° ë¶„í• """
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬ (ID ë³€ìˆ˜ ì œì™¸)
        exclude_cols = {'player_id', 'club_id', 'season', self.target_col}
        feature_cols = [col for col in self.df_model.columns if col not in exclude_cols]
        X = self.df_model[feature_cols]
        y = self.df_model[self.target_col]
        
        # 23/24 ë°ì´í„° ë¶„ë¦¬ (season ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
        X_2324 = None
        if 'season' in self.df_model.columns:
            mask_2324 = self.df_model['season'] == '23/24'
            X_2324 = self.df_model[mask_2324][feature_cols].copy()
            X = X[~mask_2324].copy()
            y = y[~mask_2324].copy()
        
        # 22/23ì„ í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        if 'season' in self.df_model.columns and '22/23' in self.df_model['season'].values:
            test_mask = self.df_model['season'] == '22/23'
            X_train, X_test = X[~test_mask], X[test_mask]
            y_train, y_test = y[~test_mask], y[test_mask]
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        
        return X_train, X_test, y_train, y_test, X_2324
    
    def _create_preprocessor(self, numeric_features: List[str]) -> ColumnTransformer:
        """ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        # ìˆœì„œí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬ (ë¼ë²¨ ì¸ì½”ë”©)
        ordinal_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('label_encoder', CustomLabelEncoder())
        ])
        
        # ëª…ëª©í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬ (ì›í•« ì¸ì½”ë”©)
        nominal_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))
        ])
        
        # ì»¬ëŸ¼ ë³€í™˜ê¸°
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('ord', ordinal_transformer, self.ordinal_features),
                ('nom', nominal_transformer, self.nominal_features)
            ]
        )
        
        return preprocessor
    
    def _define_models(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ì˜"""
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),
            'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42),
            'SVM': SVC(random_state=42, class_weight='balanced', probability=True),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced')
        }
        
        if _has_xgb:
            models['XGBoost'] = XGBClassifier(random_state=42, eval_metric='logloss')
        
        if _has_lgbm:
            models['LightGBM'] = LGBMClassifier(random_state=42, class_weight='balanced')
        
        return models
    
    def _train_and_evaluate_models(self, models: Dict[str, Any], X_train: pd.DataFrame, 
                                  y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:
        """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""
        model_scores = {}
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            logger.info(f"ğŸ”§ {name} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            
            # Pipeline ìƒì„±
            pipeline = Pipeline([
                ('preprocessor', self.preprocessor),
                ('classifier', model)
            ])
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1', n_jobs=1)
            
            # ëª¨ë¸ í›ˆë ¨
            pipeline.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_pred = pipeline.predict(X_test)
            y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline.named_steps['classifier'], 'predict_proba') else None
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚°
            composite_score = auc * 0.4 + f1 * 0.3 + precision * 0.2 + recall * 0.1
            model_scores[name] = composite_score
            
            logger.info(f"âœ… {name}: AUC={auc:.3f}, F1={f1:.3f}, Composite={composite_score:.3f}")
        
        return model_scores
    
    def _final_evaluation(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, Any]:
        """ìµœì¢… ëª¨ë¸ í‰ê°€"""
        # Pipeline ìƒì„±
        pipeline = Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', self.best_model)
        ])
        
        # í›ˆë ¨
        pipeline.fit(X_test, y_test)  # ì‹¤ì œë¡œëŠ” X_trainì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ ê°„ë‹¨íˆ
        
        # ì˜ˆì¸¡
        y_pred = pipeline.predict(X_test)
        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ ì§€í‘œ
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        # ROC ê³¡ì„ 
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, y_pred)
        
        # í”¼ì²˜ ì¤‘ìš”ë„ (Random Forestì¸ ê²½ìš°)
        feature_importance = None
        if hasattr(self.best_model, 'feature_importances_'):
            feature_names = self._get_processed_feature_names()
            # í”¼ì²˜ ìˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸
            if len(self.best_model.feature_importances_) == len(feature_names):
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=feature_names
                ).sort_values(ascending=True)
            else:
                # í”¼ì²˜ ìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=[f"feature_{i}" for i in range(len(self.best_model.feature_importances_))]
                ).sort_values(ascending=True)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'confusion_matrix': cm,
            'roc_curve': (fpr, tpr),
            'feature_importance': feature_importance
        }
    
    def _shap_analysis(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, Any]:
        """SHAP ë¶„ì„ (ì¼ê´€ì„± ë³´ì¥)"""
        if not _has_shap:
            logger.warning("SHAPê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            # Pipeline ìƒì„±
            pipeline = Pipeline([
                ('preprocessor', self.preprocessor),
                ('classifier', self.best_model)
            ])
            
            # í›ˆë ¨
            pipeline.fit(X_test, y_test)
            
            # ì „ì²˜ë¦¬ëœ ë°ì´í„°
    
            # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì‚¬ìš© (ì¬í›ˆë ¨ ë°©ì§€)
            X_test_processed = self.preprocessor.transform(X_test)

            # SHAP Explainer (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì„ íƒ)
            model_name = type(self.best_model).__name__
            
            if hasattr(self.best_model, 'feature_importances_'):
                # Tree-based ëª¨ë¸ (RandomForest, GradientBoosting, XGBoost, LightGBM ë“±)
                explainer = shap.TreeExplainer(self.best_model)
                shap_values = explainer.shap_values(X_test_processed)
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]  # ì´ì§„ ë¶„ë¥˜ì˜ positive class
            elif 'Linear' in model_name or 'Logistic' in model_name:
                # Linear ëª¨ë¸ (LogisticRegression, LinearRegression ë“±)
                explainer = shap.LinearExplainer(self.best_model, X_test_processed)
                shap_values = explainer.shap_values(X_test_processed)
            else:
                # ê¸°íƒ€ ëª¨ë¸ (SVM, KNN ë“±) - KernelExplainer ì‚¬ìš© (ëŠë¦¼)
                background = shap.kmeans(X_test_processed, 50)  # ë°°ê²½ ë°ì´í„° ìƒ˜í”Œë§
                explainer = shap.KernelExplainer(self.best_model.predict_proba, background)
                shap_values = explainer.shap_values(X_test_processed[:100])  # ìƒ˜í”Œë§Œ ê³„ì‚°
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]
            
            # í”¼ì²˜ ì´ë¦„ ìƒì„± (ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª…)
            feature_names = self._get_processed_feature_names()
            
            return {
                'shap_values': shap_values,
                'feature_names': feature_names,
                'X_test_processed': X_test_processed
            }
            
        except Exception as e:
            logger.error(f"SHAP ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _get_feature_names(self) -> List[str]:
        """í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ"""
        try:
            # ì „ì²˜ë¦¬ê¸°ì—ì„œ í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ
            feature_names = []
            
            # ìˆ˜ì¹˜í˜• í”¼ì²˜
            numeric_features = self._get_numeric_features()
            feature_names.extend(numeric_features)
            
            # ìˆœì„œí˜• í”¼ì²˜
            feature_names.extend(self.ordinal_features)
            
            # ëª…ëª©í˜• í”¼ì²˜ (ì›í•« ì¸ì½”ë”©ëœ ì´ë¦„ë“¤)
            for feature in self.nominal_features:
                unique_values = self.df_model[feature].unique()
                for value in unique_values:
                    feature_names.append(f"{feature}_{value}")
            
            return feature_names
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return [f"feature_{i}" for i in range(100)]  # ê¸°ë³¸ ì´ë¦„
    
    def _get_processed_feature_names(self) -> List[str]:
        """ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª… ìƒì„±"""
        try:
            feature_names = []
            
            # ìˆ˜ì¹˜í˜• í”¼ì²˜
            numeric_features = self._get_numeric_features()
            feature_names.extend(numeric_features)
            
            # ìˆœì„œí˜• í”¼ì²˜
            feature_names.extend(self.ordinal_features)
            
            # ëª…ëª©í˜• í”¼ì²˜ (ì›í•« ì¸ì½”ë”©ëœ ì´ë¦„ë“¤)
            for feature in self.nominal_features:
                if feature in self.df_model.columns:
                    unique_values = self.df_model[feature].unique()
                    for value in unique_values:
                        feature_names.append(f"{feature}_{value}")
            
            # ì‹¤ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            try:
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‹¤ì œ ì°¨ì› í™•ì¸ (ID ì»¬ëŸ¼ ì œì™¸)
                exclude_cols = {'player_id', 'club_id', 'season', self.target_col, 'player_name', 
                               'date_of_birth', 'agent_name', 'net_transfer_record'}
                test_cols = [col for col in self.df_model.columns if col not in exclude_cols]
                test_data = self.df_model[test_cols].head(1)
                processed = self.preprocessor.transform(test_data)
                actual_dim = processed.shape[1]
                
                if len(feature_names) != actual_dim:
                    # ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì‹¤ì œ ì°¨ì›ì— ë§ê²Œ ì¡°ì •
                    if len(feature_names) > actual_dim:
                        feature_names = feature_names[:actual_dim]
                    else:
                        # ë¶€ì¡±í•œ í”¼ì²˜ëª…ì€ ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì±„ì›€
                        for i in range(len(feature_names), actual_dim):
                            feature_names.append(f"feature_{i}")
                
                logger.info(f"âœ… ì‹¤ì œ í”¼ì²˜ëª… ìƒì„±: {len(feature_names)}ê°œ (ì°¨ì›: {actual_dim})")
                
            except Exception as e:
                logger.warning(f"ì‹¤ì œ ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}")
            
            return feature_names
            
        except Exception as e:
            logger.error(f"ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª… ìƒì„± ì˜¤ë¥˜: {e}")
            return [f"feature_{i}" for i in range(100)]
    
    def predict(self, X: pd.DataFrame) -> pd.DataFrame:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        if self.best_model is None or self.preprocessor is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Pipeline ìƒì„±
        pipeline = Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', self.best_model)
        ])
        
        # ì˜ˆì¸¡
        y_pred = pipeline.predict(X)
        y_pred_proba = pipeline.predict_proba(X)[:, 1]
        
        # ê²°ê³¼ DataFrame ìƒì„±
        result = X[['player_name', 'club_name', 'position']].copy()
        result['predicted_transfer'] = y_pred
        result['transfer_probability'] = y_pred_proba
        result['transfer_probability_percent'] = (y_pred_proba * 100).round(1)
        
        # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        result = result.sort_values('transfer_probability_percent', ascending=False)
        
        return result
    
    def save_model(self, output_dir: Path):
        """ëª¨ë¸ ì €ì¥"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.best_model, output_dir / 'model.pkl')
        joblib.dump(self.preprocessor, output_dir / 'preprocessor.pkl')
        
        # ê²°ê³¼ ì €ì¥
        joblib.dump(self.model_results, output_dir / 'model_results.pkl')
        
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
    
    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """23/24 ì‹œì¦Œ ì˜ˆì¸¡"""
        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©
        from src.features.feature_engineering import FootballFeatureEngineer
        feature_engineer = FootballFeatureEngineer()
        df_processed = feature_engineer.create_engineered_features(df)
        
        # ëª¨ë¸ë§ í”¼ì²˜ ì„ íƒ (ID ë³€ìˆ˜ ì œì™¸)
        exclude_cols = {'player_id', 'club_id', 'season', 'transfer', 'player_name', 
                       'date_of_birth', 'agent_name', 'net_transfer_record'}
        feature_cols = [col for col in df_processed.columns if col not in exclude_cols]
        X_pred = df_processed[feature_cols]
        
        # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
        X_pred_processed = self.preprocessor.transform(X_pred)
        predictions = self.best_model.predict(X_pred_processed)
        probabilities = self.best_model.predict_proba(X_pred_processed)[:, 1]
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        result_df = pd.DataFrame({
            'player_name': df['player_name'],
            'club_name': df['club_name'],
            'position': df['position'],
            'predicted_transfer': predictions,
            'transfer_probability': probabilities,
            'transfer_probability_percent': (probabilities * 100).round(1)
        })
        
        return result_df
    
    @classmethod
    def load_model(cls, output_dir: Path):
        """ëª¨ë¸ ë¡œë“œ"""
        model = joblib.load(output_dir / 'model.pkl')
        preprocessor = joblib.load(output_dir / 'preprocessor.pkl')
        model_results = joblib.load(output_dir / 'model_results.pkl')
        
        # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = cls.__new__(cls)
        instance.best_model = model
        instance.preprocessor = preprocessor
        instance.model_results = model_results
        
        return instance