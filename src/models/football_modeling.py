"""
Football Transfer Prediction - Modeling Pipeline
ì‹¤ì œ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ë§ í´ë˜ìŠ¤
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from typing import Dict, Any, List, Tuple
import logging

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€
from src.features.feature_engineering import FootballFeatureEngineer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, 
                           precision_score, recall_score, f1_score, roc_auc_score, 
                           roc_curve, precision_recall_curve, auc)
from sklearn.utils.class_weight import compute_class_weight

# Optional libraries
try:
    import xgboost as xgb
    from xgboost import XGBClassifier
    _has_xgb = True
except ImportError:
    _has_xgb = False

try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
    _has_lgbm = True
except ImportError:
    _has_lgbm = False

try:
    import shap
    _has_shap = True
except ImportError:
    _has_shap = False

logger = logging.getLogger(__name__)

class CustomLabelEncoder(BaseEstimator, TransformerMixin):
    """íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ ì¸ì½”ë”"""
    
    def __init__(self):
        self.label_encoders = {}
        self.is_fitted = False
    
    def fit(self, X, y=None):
        self.label_encoders = {}
        if hasattr(X, 'iloc'):
            for i in range(X.shape[1]):
                le = LabelEncoder()
                le.fit(X.iloc[:, i].astype(str))
                self.label_encoders[i] = le
        else:
            for i in range(X.shape[1]):
                le = LabelEncoder()
                le.fit(X[:, i].astype(str))
                self.label_encoders[i] = le
        self.is_fitted = True
        return self
    
    def transform(self, X):
        if not self.is_fitted:
            raise ValueError("LabelEncoderê°€ fitë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        X_encoded = np.zeros_like(X, dtype=float)
        
        if hasattr(X, 'iloc'):
            for i in range(X.shape[1]):
                try:
                    X_encoded[:, i] = self.label_encoders[i].transform(X.iloc[:, i].astype(str))
                except ValueError:
                    # ìƒˆë¡œìš´ ë¼ë²¨ì´ ìˆëŠ” ê²½ìš° -1ë¡œ ë§¤í•‘
                    X_encoded[:, i] = np.array([
                        self.label_encoders[i].transform([x])[0] if x in self.label_encoders[i].classes_ else -1
                        for x in X.iloc[:, i].astype(str)
                    ])
        else:
            for i in range(X.shape[1]):
                try:
                    X_encoded[:, i] = self.label_encoders[i].transform(X[:, i].astype(str))
                except ValueError:
                    X_encoded[:, i] = np.array([
                        self.label_encoders[i].transform([x])[0] if x in self.label_encoders[i].classes_ else -1
                        for x in X[:, i].astype(str)
                    ])
        return X_encoded.astype(float)

class FootballModelTrainer:
    """Football Transfer Prediction ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, df_model: pd.DataFrame, config):
        self.df_model = df_model
        self.config = config
        self.target_col = config.target_column
        self.ordinal_features = config.features_ordinal
        self.nominal_features = config.features_nominal
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.model_results = {}
        self.best_model = None
        self.preprocessor = None
        
    def run_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 0. ë°ì´í„° í’ˆì§ˆ ë° ëˆ„ìˆ˜ ê²€ì‚¬
        from src.features.feature_engineering import DataLeakageChecker
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        quality_results = DataLeakageChecker.check_data_quality(self.df_model)
        logger.info("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ")
        if quality_results['high_missing_features']:
            logger.warning(f"ë†’ì€ ê²°ì¸¡ì¹˜ í”¼ì²˜: {list(quality_results['high_missing_features'].keys())}")
        if quality_results['constant_features']:
            logger.warning(f"ìƒìˆ˜ í”¼ì²˜: {quality_results['constant_features']}")
        if quality_results['duplicate_rows'] > 0:
            logger.info(f"â„¹ï¸ ì¤‘ë³µ í–‰: {quality_results['duplicate_rows']}ê°œ (ì •ìƒì ì¸ ë°ì´í„° íŠ¹ì„±)")
        
        # ì‹œê°„ì  ëˆ„ìˆ˜ ê²€ì‚¬
        if 'season' in self.df_model.columns:
            temporal_results = DataLeakageChecker.check_temporal_leakage(
                self.df_model, 'season', self.target_col
            )
            logger.info("ğŸ” ì‹œê°„ì  ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬ ì™„ë£Œ")
        
        # í”¼ì²˜ ëˆ„ìˆ˜ ê²€ì‚¬
        feature_leakage = DataLeakageChecker.check_feature_leakage(self.df_model, self.target_col)
        if feature_leakage['suspicious_features']:
            logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í”¼ì²˜: {feature_leakage['suspicious_features']}")
        else:
            logger.info("âœ… í”¼ì²˜ ëˆ„ìˆ˜ ì—†ìŒ")
        
        # 1. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì „ì²´ ë°ì´í„°ì— ì ìš©)
        feature_engineer = FootballFeatureEngineer()
        self.df_model = feature_engineer.create_engineered_features(self.df_model)
        logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {self.df_model.shape}")
        
        # 2. ë°ì´í„° ë¶„í•  (22/23ì„ validationìœ¼ë¡œ ì‚¬ìš©)
        X_train, X_val, y_train, y_val, X_2324 = self._split_data()
        
        # 3. ì „ì²˜ë¦¬ê¸° ìƒì„± ë° í•™ìŠµ (train ë°ì´í„°ë§Œ ì‚¬ìš© - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        feature_types = feature_engineer.get_feature_types(self.df_model)
        self.preprocessor = feature_engineer.create_preprocessor(feature_types)
        self.feature_types = feature_types
        
        # train ë°ì´í„°ë¡œë§Œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ
        self.preprocessor.fit(X_train)
        logger.info("âœ… ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ (train ë°ì´í„°ë§Œ ì‚¬ìš©)")
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_train = self.preprocessor.transform(X_train)
        X_val = self.preprocessor.transform(X_val)
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì ìš© ì™„ë£Œ: train {X_train.shape}, validation {X_val.shape}")
        
        # 4. ëª¨ë¸ ì •ì˜
        models = self._define_models()
        
        # 5. ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ (validation ë°ì´í„°ë¡œ í‰ê°€)
        model_scores, model_details = self._train_and_evaluate_models(models, X_train, y_train, X_val, y_val)
        
        # 6. ìµœì  ëª¨ë¸ ì„ íƒ
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = models[best_model_name]
        
        # 6.5. ì˜¤ë²„í”¼íŒ… ê²€ì‚¬
        from src.features.feature_engineering import OverfittingChecker
        
        # í•™ìŠµ ê³¡ì„  ë¶„ì„
        learning_curve_results = OverfittingChecker.check_learning_curves(
            self.best_model, X_train, y_train, X_val, y_val
        )
        if learning_curve_results['is_overfitting']:
            logger.warning(f"âš ï¸ ì˜¤ë²„í”¼íŒ… ê°ì§€! ìµœì¢… ê°­: {learning_curve_results['final_gap']:.3f}, ìµœëŒ€ ê°­: {learning_curve_results['max_gap']:.3f}")
        else:
            logger.info(f"âœ… ì˜¤ë²„í”¼íŒ… ì—†ìŒ (ìµœì¢… ê°­: {learning_curve_results['final_gap']:.3f})")
        
        # êµì°¨ê²€ì¦ ì¼ê´€ì„± ê²€ì‚¬
        cv_results = OverfittingChecker.check_cv_consistency(self.best_model, X_train, y_train)
        if cv_results['is_stable']:
            logger.info(f"âœ… êµì°¨ê²€ì¦ ì•ˆì •ì„±: {cv_results['cv_mean']:.3f} Â± {cv_results['cv_std']:.3f}")
        else:
            logger.warning(f"âš ï¸ êµì°¨ê²€ì¦ ë¶ˆì•ˆì •: {cv_results['cv_mean']:.3f} Â± {cv_results['cv_std']:.3f}")
        
        # 7. ìµœì¢… í‰ê°€ (validation ë°ì´í„°ë¡œ í‰ê°€)
        final_results = self._final_evaluation(X_val, y_val)
        
        # 8. SHAP ë¶„ì„ (validation ë°ì´í„°ë¡œ ë¶„ì„)
        shap_results = self._shap_analysis(X_val, y_val)
        
        # 9. ê²°ê³¼ ì €ì¥
        self.model_results = {
            'model_scores': model_scores,
            'model_details': model_details,  # ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€
            'model_comparison': model_scores,  # Plotterì—ì„œ ì‚¬ìš©
            'best_model_name': best_model_name,
            'best_model': self.best_model,
            'preprocessor': self.preprocessor,
            'final_results': final_results,
            'shap_results': shap_results,
            'X_train': X_train,      # í›ˆë ¨ ë°ì´í„° ì¶”ê°€
            'y_train': y_train,      # í›ˆë ¨ íƒ€ê²Ÿ ì¶”ê°€
            'X_val': X_val,   # validation ë°ì´í„°
            'y_val': y_val,   # validation íƒ€ê²Ÿ
            'X_2324': X_2324,
            # ê²€ì‚¬ ê²°ê³¼ ì¶”ê°€
            'data_quality_results': quality_results,
            'feature_leakage_results': feature_leakage,
            'learning_curve_results': learning_curve_results,
            'cv_consistency_results': cv_results
        }
        
        # 9. ì‹œê°í™” (SHAP, í”¼ì²˜ ì¤‘ìš”ë„, í•™ìŠµ ê³¡ì„ )
        try:
            from src.visualization.plotter import ModelVisualizer
            visualizer = ModelVisualizer(self.model_results, self.output_dir)
            
            # ëª¨ë“  ì‹œê°í™” ìƒì„± (í•™ìŠµ ê³¡ì„  í¬í•¨)
            visualizer.create_all_plots()
            
            logger.info("âœ… ì‹œê°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì‹œê°í™” ì˜¤ë¥˜: {e}")
        
        logger.info("âœ… ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return self.model_results
    
    def _get_numeric_features(self) -> List[str]:
        """ìˆ˜ì¹˜í˜• í”¼ì²˜ ì¶”ì¶œ (ID ë³€ìˆ˜ ì œì™¸)"""
        all_features = set(self.df_model.columns) - {self.target_col}
        categorical_features = set(self.ordinal_features + self.nominal_features)
        # ì œì™¸í•  ID ë³€ìˆ˜ë“¤
        exclude_cols = {'player_id', 'club_id', 'season'}
        
        return [col for col in all_features if col not in categorical_features and 
                col not in exclude_cols and
                pd.api.types.is_numeric_dtype(self.df_model[col])]
    
    def _split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame]:
        """ë°ì´í„° ë¶„í•  (22/23ì„ validationìœ¼ë¡œ ì‚¬ìš©)"""
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬ (ID ë³€ìˆ˜ ì œì™¸)
        exclude_cols = {'player_id', 'club_id', 'season', self.target_col}
        feature_cols = [col for col in self.df_model.columns if col not in exclude_cols]
        X = self.df_model[feature_cols]
        y = self.df_model[self.target_col]
        
        # 23/24 ë°ì´í„° ë¶„ë¦¬ (ì˜ˆì¸¡ìš© ë°ì´í„°)
        X_2324 = None
        if 'season' in self.df_model.columns:
            mask_2324 = self.df_model['season'] == '23/24'
            X_2324 = self.df_model[mask_2324][feature_cols].copy()
            X = X[~mask_2324].copy()
            y = y[~mask_2324].copy()
        
        # 22/23ì„ validationìœ¼ë¡œ ì‚¬ìš©
        if 'season' in self.df_model.columns and '22/23' in self.df_model['season'].values:
            validation_mask = self.df_model['season'] == '22/23'
            # 22/23ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¥¼ trainìœ¼ë¡œ, 22/23ì„ validationìœ¼ë¡œ
            X_train, X_val = X[~validation_mask], X[validation_mask]
            y_train, y_val = y[~validation_mask], y[validation_mask]
            
            logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
            logger.info(f"  - Train: {X_train.shape[0]:,} rows (11-21 ì‹œì¦Œ)")
            logger.info(f"  - Validation: {X_val.shape[0]:,} rows (22/23 ì‹œì¦Œ)")
            logger.info(f"  - Prediction: {X_2324.shape[0] if X_2324 is not None else 0:,} rows (23/24 ì‹œì¦Œ)")
        else:
            # season ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ (ëœë¤ ë¶„í• ):")
            logger.info(f"  - Train: {X_train.shape[0]:,} rows")
            logger.info(f"  - Validation: {X_val.shape[0]:,} rows")
        
        return X_train, X_val, y_train, y_val, X_2324
    
    def _create_preprocessor(self, numeric_features: List[str]) -> ColumnTransformer:
        """ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        # ìˆœì„œí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬ (ë¼ë²¨ ì¸ì½”ë”©)
        ordinal_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('label_encoder', CustomLabelEncoder())
        ])
        
        # ëª…ëª©í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬ (ì›í•« ì¸ì½”ë”©)
        nominal_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))
        ])
        
        # ì»¬ëŸ¼ ë³€í™˜ê¸°
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('ord', ordinal_transformer, self.ordinal_features),
                ('nom', nominal_transformer, self.nominal_features)
            ]
        )
        
        return preprocessor
    
    def _define_models(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ì˜"""
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),
            'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42),
            'SVM': SVC(random_state=42, class_weight='balanced', probability=True),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced')
        }
        
        if _has_xgb:
            models['XGBoost'] = XGBClassifier(random_state=42, eval_metric='logloss')
        
        if _has_lgbm:
            models['LightGBM'] = LGBMClassifier(random_state=42, class_weight='balanced')
        
        return models
    
    def _train_and_evaluate_models(self, models: Dict[str, Any], X_train: pd.DataFrame, 
                                  y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""
        model_scores = {}
        model_details = {}  # ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ì €ì¥
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            logger.info(f"ğŸ”§ {name} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            
            # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì´ë¯€ë¡œ ëª¨ë¸ë§Œ ì‚¬ìš©
            # êµì°¨ ê²€ì¦ (ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ ì§ì ‘ ìˆ˜í–‰)
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1', n_jobs=-1)
            
            # ëª¨ë¸ í›ˆë ¨
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred)
            recall = recall_score(y_val, y_pred)
            f1 = f1_score(y_val, y_pred)
            auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else 0
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê· )
            # Precision 40% + F1 30% + Accuracy 20% + Recall 10%
            composite_score = (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            model_scores[name] = composite_score
            
            # ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ì €ì¥
            model_details[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'composite_score': composite_score,
                'cv_f1_mean': cv_scores.mean(),
                'cv_f1_std': cv_scores.std()
            }
            
            logger.info(f"âœ… {name}: AUC={auc:.3f}, F1={f1:.3f}, Composite={composite_score:.3f}")
        
        return model_scores, model_details
    
    def _final_evaluation(self, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """ìµœì¢… ëª¨ë¸ í‰ê°€"""
        # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš© (ì „ì²˜ë¦¬ëœ ë°ì´í„°)
        # ì˜ˆì¸¡
        y_pred = self.best_model.predict(X_val)
        y_pred_proba = self.best_model.predict_proba(X_val)[:, 1] if hasattr(self.best_model, 'predict_proba') else None
        
        # ì„±ëŠ¥ ì§€í‘œ
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        # ROC ê³¡ì„ 
        fpr, tpr, _ = roc_curve(y_val, y_pred_proba)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_val, y_pred)
        
        # í”¼ì²˜ ì¤‘ìš”ë„ (Random Forestì¸ ê²½ìš°)
        feature_importance = None
        if hasattr(self.best_model, 'feature_importances_'):
            feature_names = self._get_processed_feature_names()
            # í”¼ì²˜ ìˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸
            if len(self.best_model.feature_importances_) == len(feature_names):
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=feature_names
                ).sort_values(ascending=True)
            else:
                # í”¼ì²˜ ìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=[f"feature_{i}" for i in range(len(self.best_model.feature_importances_))]
                ).sort_values(ascending=True)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'confusion_matrix': cm,
            'roc_curve': (fpr, tpr),
            'feature_importance': feature_importance
        }
    
    def _shap_analysis(self, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """SHAP ë¶„ì„ (ì¼ê´€ì„± ë³´ì¥)"""
        if not _has_shap:
            logger.warning("SHAPê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì‚¬ìš© (ì¬í›ˆë ¨ ë°©ì§€)
            # X_valëŠ” ì´ë¯¸ ì „ì²˜ë¦¬ëœ ìƒíƒœ
            X_val_processed = X_val

            # SHAP Explainer (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì„ íƒ)
            model_name = type(self.best_model).__name__
            
            if hasattr(self.best_model, 'feature_importances_'):
                # Tree-based ëª¨ë¸ (RandomForest, GradientBoosting, XGBoost, LightGBM ë“±)
                explainer = shap.TreeExplainer(self.best_model)
                shap_values = explainer.shap_values(X_val_processed)
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]  # ì´ì§„ ë¶„ë¥˜ì˜ positive class
            elif 'Linear' in model_name or 'Logistic' in model_name:
                # Linear ëª¨ë¸ (LogisticRegression, LinearRegression ë“±)
                explainer = shap.LinearExplainer(self.best_model, X_val_processed)
                shap_values = explainer.shap_values(X_val_processed)
            else:
                # ê¸°íƒ€ ëª¨ë¸ (SVM, KNN ë“±) - KernelExplainer ì‚¬ìš© (ëŠë¦¼)
                background = shap.kmeans(X_val_processed, 50)  # ë°°ê²½ ë°ì´í„° ìƒ˜í”Œë§
                explainer = shap.KernelExplainer(self.best_model.predict_proba, background)
                shap_values = explainer.shap_values(X_val_processed[:100])  # ìƒ˜í”Œë§Œ ê³„ì‚°
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]
            
            # í”¼ì²˜ ì´ë¦„ ìƒì„± (ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª…)
            feature_names = self._get_processed_feature_names()
            
            return {
                'shap_values': shap_values,
                'feature_names': feature_names,
                'X_val_processed': X_val_processed
            }
            
        except Exception as e:
            logger.error(f"SHAP ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _get_feature_names(self) -> List[str]:
        """í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ"""
        try:
            # ì „ì²˜ë¦¬ê¸°ì—ì„œ í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ
            feature_names = []
            
            # ìˆ˜ì¹˜í˜• í”¼ì²˜
            numeric_features = self._get_numeric_features()
            feature_names.extend(numeric_features)
            
            # ìˆœì„œí˜• í”¼ì²˜
            feature_names.extend(self.ordinal_features)
            
            # ëª…ëª©í˜• í”¼ì²˜ (ì›í•« ì¸ì½”ë”©ëœ ì´ë¦„ë“¤)
            for feature in self.nominal_features:
                unique_values = self.df_model[feature].unique()
                for value in unique_values:
                    feature_names.append(f"{feature}_{value}")
            
            return feature_names
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return [f"feature_{i}" for i in range(100)]  # ê¸°ë³¸ ì´ë¦„
    
    def _get_processed_feature_names(self) -> List[str]:
        """ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª… ìƒì„±"""
        try:
            feature_names = []
            
            # ìˆ˜ì¹˜í˜• í”¼ì²˜
            numeric_features = self._get_numeric_features()
            feature_names.extend(numeric_features)
            
            # ìˆœì„œí˜• í”¼ì²˜
            feature_names.extend(self.ordinal_features)
            
            # ëª…ëª©í˜• í”¼ì²˜ (ì›í•« ì¸ì½”ë”©ëœ ì´ë¦„ë“¤)
            for feature in self.nominal_features:
                if feature in self.df_model.columns:
                    unique_values = self.df_model[feature].unique()
                    for value in unique_values:
                        feature_names.append(f"{feature}_{value}")
            
            # ì‹¤ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            try:
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‹¤ì œ ì°¨ì› í™•ì¸ (ID ì»¬ëŸ¼ ì œì™¸)
                exclude_cols = {'player_id', 'club_id', 'season', self.target_col, 'player_name', 
                               'date_of_birth', 'agent_name', 'net_transfer_record'}
                test_cols = [col for col in self.df_model.columns if col not in exclude_cols]
                test_data = self.df_model[test_cols].head(1)
                processed = self.preprocessor.transform(test_data)
                actual_dim = processed.shape[1]
                
                if len(feature_names) != actual_dim:
                    # ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì‹¤ì œ ì°¨ì›ì— ë§ê²Œ ì¡°ì •
                    if len(feature_names) > actual_dim:
                        feature_names = feature_names[:actual_dim]
                    else:
                        # ë¶€ì¡±í•œ í”¼ì²˜ëª…ì€ ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì±„ì›€
                        for i in range(len(feature_names), actual_dim):
                            feature_names.append(f"feature_{i}")
                
                logger.info(f"âœ… ì‹¤ì œ í”¼ì²˜ëª… ìƒì„±: {len(feature_names)}ê°œ (ì°¨ì›: {actual_dim})")
                
            except Exception as e:
                logger.warning(f"ì‹¤ì œ ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}")
            
            return feature_names
            
        except Exception as e:
            logger.error(f"ì „ì²˜ë¦¬ í›„ í”¼ì²˜ëª… ìƒì„± ì˜¤ë¥˜: {e}")
            return [f"feature_{i}" for i in range(100)]
    
    
    def save_model(self, output_dir: Path):
        """ëª¨ë¸ ì €ì¥"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.best_model, output_dir / 'model.pkl')
        joblib.dump(self.preprocessor, output_dir / 'preprocessor.pkl')
        
        # ê²°ê³¼ ì €ì¥
        joblib.dump(self.model_results, output_dir / 'model_results.pkl')
        
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
    
    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """23/24 ì‹œì¦Œ ì˜ˆì¸¡"""
        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©
        from src.features.feature_engineering import FootballFeatureEngineer
        feature_engineer = FootballFeatureEngineer()
        df_processed = feature_engineer.create_engineered_features(df)
        
        # ëª¨ë¸ë§ í”¼ì²˜ ì„ íƒ (ID ë³€ìˆ˜ ì œì™¸)
        exclude_cols = {'player_id', 'club_id', 'season', 'transfer', 'player_name', 
                       'date_of_birth', 'agent_name', 'net_transfer_record'}
        feature_cols = [col for col in df_processed.columns if col not in exclude_cols]
        X_pred = df_processed[feature_cols]
        
        # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
        X_pred_processed = self.preprocessor.transform(X_pred)
        predictions = self.best_model.predict(X_pred_processed)
        probabilities = self.best_model.predict_proba(X_pred_processed)[:, 1]
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        result_df = pd.DataFrame({
            'player_name': df['player_name'],
            'club_name': df['club_name'],
            'position': df['position'],
            'predicted_transfer': predictions,
            'transfer_probability': probabilities,
            'transfer_probability_percent': (probabilities * 100).round(1)
        })
        
        return result_df
    
    @classmethod
    def load_model(cls, output_dir: Path):
        """ëª¨ë¸ ë¡œë“œ"""
        model = joblib.load(output_dir / 'model.pkl')
        preprocessor = joblib.load(output_dir / 'preprocessor.pkl')
        model_results = joblib.load(output_dir / 'model_results.pkl')
        
        # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = cls.__new__(cls)
        instance.best_model = model
        instance.preprocessor = preprocessor
        instance.model_results = model_results
        
        return instance