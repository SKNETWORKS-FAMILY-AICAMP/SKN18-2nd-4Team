"""
Football Transfer Prediction - Modeling Pipeline
ì‹¤ì œ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ë§ í´ë˜ìŠ¤
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from typing import Dict, Any, List, Tuple
import logging

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€
from src.features.feature_engineering import FootballFeatureEngineer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, 
                           precision_score, recall_score, f1_score, roc_auc_score, 
                           roc_curve, precision_recall_curve)
from sklearn.metrics import auc as sklearn_auc
from sklearn.utils.class_weight import compute_class_weight

# Optional libraries
try:
    import xgboost as xgb
    from xgboost import XGBClassifier
    _has_xgb = True
except ImportError:
    _has_xgb = False

try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
    _has_lgbm = True
except ImportError:
    _has_lgbm = False

try:
    import shap
    _has_shap = True
except ImportError:
    _has_shap = False

logger = logging.getLogger(__name__)

class CustomLabelEncoder(BaseEstimator, TransformerMixin):
    """íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ ì¸ì½”ë”"""
    
    def __init__(self):
        self.label_encoders = {}
        self.is_fitted = False
    
    def fit(self, X, y=None):
        X = X.copy()
        for i in range(X.shape[1]):
            le = LabelEncoder()
            le.fit(X[:, i].astype(str))
            self.label_encoders[i] = le
        self.is_fitted = True
        return self
    
    def transform(self, X):
        if not self.is_fitted:
            raise ValueError("LabelEncoder must be fitted before transform")
        X = X.copy()
        X_encoded = np.zeros_like(X, dtype=float)
        for i in range(X.shape[1]):
            le = self.label_encoders[i]
            X_encoded[:, i] = le.transform([
                x if pd.notna(x) and x != '' else 'unknown'
                for x in X[:, i].astype(str)
            ])
        return X_encoded.astype(float)

class FootballModelTrainer:
    """Football Transfer Prediction ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, train_data: pd.DataFrame, valid_data: pd.DataFrame, test_data: pd.DataFrame, pred_data: pd.DataFrame, config):
        self.train_data = train_data
        self.valid_data = valid_data
        self.test_data = test_data
        self.pred_data = pred_data
        self.config = config
        self.target_col = config.target_column
        self.ordinal_features = config.features_ordinal
        self.nominal_features = config.features_nominal
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.model_results = {}
        self.best_model = None
        self.preprocessor = None
        self.output_dir = Path(config.output_dir)
        
    def run_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 0. ë°ì´í„° í’ˆì§ˆ ë° ëˆ„ìˆ˜ ê²€ì‚¬
        from src.features.feature_engineering import DataLeakageChecker
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        quality_results = DataLeakageChecker.check_data_quality(self.train_data)
        logger.info("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ")
        if quality_results['high_missing_features']:
            logger.warning(f"ë†’ì€ ê²°ì¸¡ì¹˜ í”¼ì²˜: {list(quality_results['high_missing_features'].keys())}")
        if quality_results['constant_features']:
            logger.warning(f"ìƒìˆ˜ í”¼ì²˜: {quality_results['constant_features']}")
        if quality_results['duplicate_rows'] > 0:
            logger.info(f"â„¹ï¸ ì¤‘ë³µ í–‰: {quality_results['duplicate_rows']}ê°œ (ì •ìƒì ì¸ ë°ì´í„° íŠ¹ì„±)")
        
        # ì‹œê°„ì  ëˆ„ìˆ˜ ê²€ì‚¬
        if 'season' in self.train_data.columns:
            temporal_results = DataLeakageChecker.check_temporal_leakage(
                self.train_data, 'season', self.target_col
            )
            logger.info("ğŸ” ì‹œê°„ì  ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬ ì™„ë£Œ")
        
        # í”¼ì²˜ ëˆ„ìˆ˜ ê²€ì‚¬
        feature_leakage = DataLeakageChecker.check_feature_leakage(self.train_data, self.target_col)
        if feature_leakage['suspicious_features']:
            logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í”¼ì²˜: {feature_leakage['suspicious_features']}")
        else:
            logger.info("âœ… í”¼ì²˜ ëˆ„ìˆ˜ ì—†ìŒ")
        
        # 1. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ê° ë°ì´í„°ì— ê°œë³„ ì ìš©)
        feature_engineer = FootballFeatureEngineer()
        self.train_data = feature_engineer.create_engineered_features(self.train_data)
        self.valid_data = feature_engineer.create_engineered_features(self.valid_data)
        self.test_data = feature_engineer.create_engineered_features(self.test_data)
        self.pred_data = feature_engineer.create_engineered_features(self.pred_data)
        logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: train {self.train_data.shape}, valid {self.valid_data.shape}, test {self.test_data.shape}, pred {self.pred_data.shape}")
        
        # 2. ë°ì´í„° ë¶„í• 
        X_train, y_train, X_val, y_val, X_test, y_test = self._split_data()
        
        # 24/25 ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„
        X_2425 = self._prepare_prediction_data()
        
        # 3. ì „ì²˜ë¦¬ê¸° ìƒì„± ë° í•™ìŠµ (train ë°ì´í„°ë§Œ ì‚¬ìš© - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        self.preprocessor = self._create_preprocessor()
        self.preprocessor.fit(X_train)
        logger.info("âœ… ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ")
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_train = self.preprocessor.transform(X_train)
        X_val = self.preprocessor.transform(X_val)
        X_test = self.preprocessor.transform(X_test)
        X_2425 = self.preprocessor.transform(X_2425)
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì ìš© ì™„ë£Œ: train {X_train.shape}, valid {X_val.shape}, test {X_test.shape}, pred {X_2425.shape}")
        
        # 4. 8ê°œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        models = self._define_models()
        model_scores, model_details = self._train_and_evaluate_models(models, X_train, y_train, X_val, y_val)
        
        # 5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = models[best_model_name]
        logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (ì ìˆ˜: {model_scores[best_model_name]:.4f})")
        
        # 6. ìµœì¢… í‰ê°€
        final_results = self._final_evaluation(X_val, y_val)
        
        # 7. 24/25 ì˜ˆì¸¡
        predictions_2425 = self._predict_2425(X_2425)
        
        # 8. í”¼ì²˜ ì¤‘ìš”ë„ ì²˜ë¦¬
        feature_importance = None
        if hasattr(self.best_model, 'feature_importances_'):
            feature_names = self._get_processed_feature_names()
            # í”¼ì²˜ ìˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸
            if len(self.best_model.feature_importances_) == len(feature_names):
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=feature_names
                ).sort_values(ascending=True)
            else:
                # í”¼ì²˜ ìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                feature_importance = pd.Series(
                    self.best_model.feature_importances_,
                    index=[f"feature_{i}" for i in range(len(self.best_model.feature_importances_))]
                ).sort_values(ascending=True)
        
        # 9. ì˜¤ë²„í”¼íŒ… ë¶„ì„ (ìƒìœ„ 3ê°œ ëª¨ë¸)
        logger.info("ğŸ” ì˜¤ë²„í”¼íŒ… ë¶„ì„ ì‹œì‘")
        from src.features.feature_engineering import OverfittingChecker
        
        # ìƒìœ„ 3ê°œ ëª¨ë¸ì— ëŒ€í•´ ì˜¤ë²„í”¼íŒ… ë¶„ì„
        top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]
        learning_curve_results = {}
        
        for model_name, score in top_models:
            if model_name in model_details:
                model = model_details[model_name]['model']
                try:
                    # ì˜¤ë²„í”¼íŒ… ë¶„ì„ ìˆ˜í–‰
                    lc_result = OverfittingChecker.check_learning_curves(
                        model, X_train_processed, y_train, X_val_processed, y_val
                    )
                    learning_curve_results[model_name] = lc_result
                    logger.info(f"âœ… {model_name} ì˜¤ë²„í”¼íŒ… ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ {model_name} ì˜¤ë²„í”¼íŒ… ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # 10. ê²°ê³¼ ì €ì¥
        self.model_results = {
            'X_train': X_train,
            'y_train': y_train,
            'X_val': X_val,
            'y_val': y_val,
            'X_test': X_test,
            'y_test': y_test,
            'X_2425': X_2425,
            'predictions_2425': predictions_2425,
            'model_scores': model_scores,
            'model_details': model_details,
            'best_model_name': best_model_name,
            'best_model': self.best_model,
            'preprocessor': self.preprocessor,
            'final_results': final_results,
            'feature_importance': feature_importance,
            'learning_curve_results': learning_curve_results
        }
        
        # 11. ì‹œê°í™” (SHAP, í”¼ì²˜ ì¤‘ìš”ë„, í•™ìŠµ ê³¡ì„ )
        try:
            from src.visualization.plotter import ModelVisualizer
            visualizer = ModelVisualizer(self.model_results, self.output_dir)
            
            # ëª¨ë“  ì‹œê°í™” ìƒì„± (í•™ìŠµ ê³¡ì„  í¬í•¨)
            visualizer.create_all_plots()
            
            logger.info("âœ… ì‹œê°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì‹œê°í™” ì˜¤ë¥˜: {e}")
        
        logger.info("âœ… ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return self.model_results
    
    def _get_numeric_features(self) -> List[str]:
        """ìˆ˜ì¹˜í˜• í”¼ì²˜ ì¶”ì¶œ (ID ë³€ìˆ˜ ì œì™¸)"""
        all_features = set(self.train_data.columns) - {self.target_col}
        categorical_features = set(self.ordinal_features + self.nominal_features)
        # ì œì™¸í•  ID ë³€ìˆ˜ë“¤
        exclude_cols = {'player_id', 'club_id', 'season'}
        return [col for col in all_features if col not in categorical_features and 
                col not in exclude_cols and
                pd.api.types.is_numeric_dtype(self.train_data[col])]
    
    def _split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.DataFrame]:
        """ë°ì´í„° ë¶„í•  (4ê°œ ë°ì´í„°ì…‹ ì‚¬ìš©)"""
        # Train ë°ì´í„°
        X_train = self.train_data.drop(columns=[self.target_col])
        y_train = self.train_data[self.target_col]
        
        # Valid ë°ì´í„°
        X_val = self.valid_data.drop(columns=[self.target_col])
        y_val = self.valid_data[self.target_col]
        
        # Test ë°ì´í„°
        X_test = self.test_data.drop(columns=[self.target_col])
        y_test = self.test_data[self.target_col]
        
        logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ: train {X_train.shape}, valid {X_val.shape}, test {X_test.shape}")
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def _prepare_prediction_data(self) -> pd.DataFrame:
        """24/25 ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„"""
        X_2425 = self.pred_data.copy()
        # target ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±° (pred ë°ì´í„°ëŠ” targetì´ ì—†ì„ ìˆ˜ ìˆìŒ)
        if self.target_col in X_2425.columns:
            X_2425 = X_2425.drop(columns=[self.target_col])
        logger.info(f"ğŸ“Š 24/25 ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„: {X_2425.shape}")
        return X_2425
    
    def _create_preprocessor(self) -> ColumnTransformer:
        """ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì „ì²˜ë¦¬
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        # ë²”ì£¼í˜• í”¼ì²˜ ì „ì²˜ë¦¬
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        # ìˆœì„œí˜• í”¼ì²˜ ì „ì²˜ë¦¬
        ordinal_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('label', CustomLabelEncoder())
        ])
        
        # ì»¬ëŸ¼ ë¶„ë¥˜
        exclude_features = self.ordinal_features + self.nominal_features + [self.target_col] + self.config.features_numeric_exclude
        numeric_features = [col for col in self.train_data.columns 
                          if col not in exclude_features]
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, self.nominal_features),
                ('ord', ordinal_transformer, self.ordinal_features)
            ]
        )
        
        return preprocessor
    
    def _get_processed_feature_names(self) -> List[str]:
        """ì „ì²˜ë¦¬ëœ í”¼ì²˜ ì´ë¦„ë“¤ ë°˜í™˜"""
        feature_names = []
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜
        exclude_features = self.ordinal_features + self.nominal_features + [self.target_col] + self.config.features_numeric_exclude
        numeric_features = [col for col in self.train_data.columns if col not in exclude_features]
        feature_names.extend(numeric_features)
        
        # ë²”ì£¼í˜• í”¼ì²˜ (ì›í•« ì¸ì½”ë”© í›„)
        if hasattr(self.preprocessor, 'named_transformers_'):
            cat_transformer = self.preprocessor.named_transformers_['cat']
            if hasattr(cat_transformer, 'get_feature_names_out'):
                cat_features = cat_transformer.get_feature_names_out(self.nominal_features)
                feature_names.extend(cat_features)
            else:
                # ì›í•« ì¸ì½”ë”©ëœ í”¼ì²˜ ì´ë¦„ ìƒì„±
                for feature in self.nominal_features:
                    if feature in self.train_data.columns:
                        unique_values = self.train_data[feature].dropna().unique()
                        for value in unique_values:
                            feature_names.append(f"{feature}_{value}")
        
        # ìˆœì„œí˜• í”¼ì²˜
        feature_names.extend(self.ordinal_features)
        
        return feature_names
    
    def _define_models(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ì˜"""
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),
            'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42),
            'SVM': SVC(random_state=42, class_weight='balanced', probability=True),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced')
        }
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • ê°’ ê³„ì‚°
        pos_weight = (self.train_data[self.target_col].shape[0] - self.train_data[self.target_col].sum()) / max(1, self.train_data[self.target_col].sum()) if self.train_data[self.target_col].sum() > 0 else 1.0
        
        # XGBoost
        if _has_xgb:
            models['XGBoost'] = XGBClassifier(
                random_state=42,
                n_estimators=300,
                learning_rate=0.05,
                max_depth=5,
                subsample=0.9,
                colsample_bytree=0.9,
                eval_metric='logloss',
                tree_method='hist',
                scale_pos_weight=float(pos_weight),
                n_jobs=-1
            )
        
        # LightGBM
        if _has_lgbm:
            models['LightGBM'] = LGBMClassifier(
                random_state=42,
                n_estimators=500,
                learning_rate=0.05,
                max_depth=-1,
                subsample=0.9,
                colsample_bytree=0.9,
                class_weight='balanced',
                n_jobs=-1
            )
        
        return models
    
    def _train_and_evaluate_models(self, models: Dict[str, Any], X_train: pd.DataFrame, 
                                  y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
        logger.info("ğŸ¤– 8ê°œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘")
        
        model_scores = {}
        model_details = {}
        
        # êµì°¨ ê²€ì¦ ì„¤ì •
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            logger.info(f"ğŸ”„ {name} í•™ìŠµ ì¤‘...")
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, zero_division=0)
            recall = recall_score(y_val, y_pred, zero_division=0)
            f1 = f1_score(y_val, y_pred, zero_division=0)
            auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else 0
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê· )
            # Precision 40% + F1 30% + Accuracy 20% + Recall 10%
            composite_score = (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            model_scores[name] = composite_score
            
            # ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ì €ì¥
            model_details[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'composite_score': composite_score,
                'cv_f1_mean': cv_scores.mean(),
                'cv_f1_std': cv_scores.std()
            }
            
            logger.info(f"âœ… {name}: AUC={auc:.3f}, F1={f1:.3f}, Composite={composite_score:.3f}")
        
        return model_scores, model_details
    
    def _final_evaluation(self, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """ìµœì¢… í‰ê°€"""
        logger.info("ğŸ“Š ìµœì¢… í‰ê°€ ìˆ˜í–‰")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        y_pred = self.best_model.predict(X_val)
        y_pred_proba = self.best_model.predict_proba(X_val)[:, 1] if hasattr(self.best_model, 'predict_proba') else None
        
        # ì„±ëŠ¥ ì§€í‘œ
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred, zero_division=0)
        recall = recall_score(y_val, y_pred, zero_division=0)
        f1 = f1_score(y_val, y_pred, zero_division=0)
        auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else 0
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(y_val, y_pred, output_dict=True)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_val, y_pred)
        
        # ROC ê³¡ì„ 
        fpr, tpr, _ = roc_curve(y_val, y_pred_proba) if y_pred_proba is not None else ([], [], [])
        roc_auc_value = sklearn_auc(fpr, tpr) if len(fpr) > 0 else 0
        
        # Precision-Recall ê³¡ì„ 
        precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_pred_proba) if y_pred_proba is not None else ([], [], [])
        pr_auc_score = sklearn_auc(recall_curve, precision_curve) if len(precision_curve) > 0 else 0
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'roc_auc': roc_auc_value,
            'pr_auc': pr_auc_score,
            'classification_report': report,
            'confusion_matrix': cm,
            'fpr': fpr,
            'tpr': tpr,
            'precision_curve': precision_curve,
            'recall_curve': recall_curve
        }
        
        logger.info(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: Accuracy={accuracy:.3f}, Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, AUC={auc:.3f}")
        return results
    
    def _predict_2425(self, X_2425: pd.DataFrame) -> np.ndarray:
        """24/25 ì‹œì¦Œ ì˜ˆì¸¡"""
        logger.info("ğŸ”® 24/25 ì‹œì¦Œ ì˜ˆì¸¡ ìˆ˜í–‰")
        
        predictions = self.best_model.predict(X_2425)
        prediction_proba = self.best_model.predict_proba(X_2425)[:, 1] if hasattr(self.best_model, 'predict_proba') else None
        
        logger.info(f"âœ… 24/25 ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì„ ìˆ˜, ì´ì  ì˜ˆìƒ {predictions.sum()}ëª…")
        
        return {
            'predictions': predictions,
            'probabilities': prediction_proba
        }
    
    def predict(self, pred_data: pd.DataFrame) -> pd.DataFrame:
        """24/25 ì‹œì¦Œ ì˜ˆì¸¡ (pred_df ì‚¬ìš©)"""
        logger.info("ğŸ”® 24/25 ì‹œì¦Œ ì˜ˆì¸¡ ì‹œì‘")
        
        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©
        feature_engineer = FootballFeatureEngineer()
        pred_data_processed = feature_engineer.create_engineered_features(pred_data)
        
        # target ì»¬ëŸ¼ ì œê±°
        if self.target_col in pred_data_processed.columns:
            X_pred = pred_data_processed.drop(columns=[self.target_col])
        else:
            X_pred = pred_data_processed
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_pred_processed = self.preprocessor.transform(X_pred)
        
        # ì˜ˆì¸¡
        predictions_binary = self.best_model.predict(X_pred_processed)
        predictions_proba = self.best_model.predict_proba(X_pred_processed)[:, 1] if hasattr(self.best_model, 'predict_proba') else None
        
        # ê²°ê³¼ DataFrame ìƒì„±
        result_df = pred_data_processed[['player_name', 'position', 'club_name']].copy()
        result_df['transfer_prediction'] = predictions_binary
        if predictions_proba is not None:
            result_df['transfer_probability'] = predictions_proba
        else:
            result_df['transfer_probability'] = predictions_binary.astype(float)
        
        logger.info(f"âœ… 24/25 ì˜ˆì¸¡ ì™„ë£Œ: {len(result_df)}ê°œ ì„ ìˆ˜, ì´ì  ì˜ˆìƒ {predictions_binary.sum()}ëª…")
        return result_df
    
    def save_model(self, output_dir: Path):
        """ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥"""
        logger.info("ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥")
        
        # ëª¨ë¸ ì €ì¥
        model_path = output_dir / "best_tuned_model.pkl"
        joblib.dump(self.best_model, model_path)
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # ì „ì²˜ë¦¬ê¸° ì €ì¥
        preprocessor_path = output_dir / "preprocessor.pkl"
        joblib.dump(self.preprocessor, preprocessor_path)
        logger.info(f"âœ… ì „ì²˜ë¦¬ê¸° ì €ì¥ ì™„ë£Œ: {preprocessor_path}")
        
        # ëª¨ë¸ ê²°ê³¼ ì €ì¥
        results_path = output_dir / "model_results.pkl"
        joblib.dump(self.model_results, results_path)
        logger.info(f"âœ… ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")