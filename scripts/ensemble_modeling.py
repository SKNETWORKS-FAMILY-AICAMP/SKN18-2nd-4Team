#!/usr/bin/env python3
"""
ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•
- Voting Classifier
- Stacking Classifier
- Bagging ë°©ë²•
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import joblib

from sklearn.ensemble import VotingClassifier, StackingClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, f1_score, roc_auc_score

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# LightGBM import
try:
    from lightgbm import LGBMClassifier
    _has_lgbm = True
except ImportError:
    _has_lgbm = False

def ensemble_modeling():
    """ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•"""
    logger.info("ğŸ¤ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì‹œì‘")
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        from src.data.data_loader_new import DataLoaderNew
        from src.utils.config import Config
        
        config = Config("config_final.yaml")
        data_loader = DataLoaderNew(config)
        train_df, test_df = data_loader.load_all_data()
        
        print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
        print(f"  - Train: {train_df.shape[0]:,} rows")
        print(f"  - Test: {test_df.shape[0]:,} rows")
        
        # 2. ê¸°ë³¸ ëª¨ë¸ë§ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        from src.models.football_modeling import FootballModelTrainer
        
        # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸° (ëª¨ë¸ë§ìš©)
        all_data = pd.concat([train_df, test_df], ignore_index=True)
        
        # ê¸°ë³¸ ëª¨ë¸ë§ ê²°ê³¼ ì¬ì‚¬ìš© (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)
        outputs_dir = Path(config.output_dir)
        model_results_path = outputs_dir / "model_results.pkl"
        
        if model_results_path.exists():
            logger.info("ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ë§ ê²°ê³¼ ì¬ì‚¬ìš© (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)")
            model_results = joblib.load(model_results_path)
        else:
            logger.info("ğŸš€ ê¸°ë³¸ ëª¨ë¸ë§ ê²°ê³¼ê°€ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤")
            model_trainer = FootballModelTrainer(all_data, config)
            model_results = model_trainer.run_pipeline()
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        X_val = model_results['X_val']  # validation ë°ì´í„° ì‚¬ìš©
        y_val = model_results['y_val']
        X_train = model_results['X_train']
        y_train = model_results['y_train']
        preprocessor = model_results['preprocessor']
        
        # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ëª…í™•í•œ ë³€ìˆ˜ëª…ìœ¼ë¡œ í• ë‹¹
        X_train_processed = X_train  # ì „ì²˜ë¦¬ ì™„ë£Œëœ train ë°ì´í„°
        X_val_processed = X_val      # ì „ì²˜ë¦¬ ì™„ë£Œëœ validation ë°ì´í„°
        
        # 4. êµì°¨ ê²€ì¦ ì„¤ì •
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        from sklearn.metrics import f1_score, roc_auc_score
        f1_scorer = make_scorer(f1_score)
        
        # 5. ê¸°ë³¸ ëª¨ë¸ë“¤ ì •ì˜
        base_models = [
            ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, 
                                       min_samples_split=10, min_samples_leaf=4,
                                       class_weight='balanced', random_state=42)),
            ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.05,
                                            max_depth=5, subsample=0.8, random_state=42)),
            ('lr', LogisticRegression(C=0.1, class_weight='balanced', 
                                    random_state=42, max_iter=1000))
        ]
        
        if _has_lgbm:
            base_models.append(('lgb', LGBMClassifier(n_estimators=200, learning_rate=0.05,
                                                    num_leaves=31, max_depth=5,
                                                    class_weight='balanced', random_state=42)))
        
        # 6. ì•™ìƒë¸” ëª¨ë¸ ì •ì˜
        ensemble_models = {}
        
        # Voting Classifier (Hard)
        ensemble_models['Voting (Hard)'] = VotingClassifier(
            estimators=base_models,
            voting='hard'
        )
        
        # Voting Classifier (Soft)
        ensemble_models['Voting (Soft)'] = VotingClassifier(
            estimators=base_models,
            voting='soft'
        )
        
        # Stacking Classifier
        ensemble_models['Stacking'] = StackingClassifier(
            estimators=base_models,
            final_estimator=LogisticRegression(C=0.1, class_weight='balanced', 
                                             random_state=42, max_iter=1000),
            cv=3
        )
        
        # Bagging with Random Forest
        ensemble_models['Bagging (RF)'] = BaggingClassifier(
            estimator=RandomForestClassifier(n_estimators=50, max_depth=8,
                                                class_weight='balanced', random_state=42),
            n_estimators=10,
            random_state=42
        )
        
        # Bagging with Gradient Boosting
        ensemble_models['Bagging (GB)'] = BaggingClassifier(
            estimator=GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,
                                                    max_depth=4, random_state=42),
            n_estimators=10,
            random_state=42
        )
        
        # 7. ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        ensemble_results = {}
        
        for model_name, model in ensemble_models.items():
            logger.info(f"ğŸ¤ {model_name} í›ˆë ¨ ì‹œì‘")
            
            # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš© (Pipeline ë¶ˆí•„ìš”)
            # êµì°¨ ê²€ì¦
            # ë³µí•©ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ êµì°¨ ê²€ì¦ (Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê· )
            def composite_scorer(y_true, y_pred):
                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
                accuracy = accuracy_score(y_true, y_pred)
                precision = precision_score(y_true, y_pred, zero_division=0)
                recall = recall_score(y_true, y_pred, zero_division=0)
                f1 = f1_score(y_true, y_pred, zero_division=0)
                return (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            
            composite_scorer_func = make_scorer(composite_scorer)
            cv_scores = cross_val_score(model, X_train_processed, y_train, cv=cv, scoring=composite_scorer_func)
            
            # ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨
            model.fit(X_train_processed, y_train)
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_val_processed)
            
            # predict_proba ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (Hard Votingì€ ë¶ˆê°€ëŠ¥)
            try:
                y_pred_proba = model.predict_proba(X_val_processed)[:, 1]
                has_proba = True
            except AttributeError:
                y_pred_proba = None
                has_proba = False
            
            # ì„±ëŠ¥ í‰ê°€
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred)
            recall = recall_score(y_val, y_pred)
            f1 = f1_score(y_val, y_pred)
            auc = roc_auc_score(y_val, y_pred_proba) if has_proba else 0
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê· )
            # Precision 40% + F1 30% + Accuracy 20% + Recall 10%
            composite_score = (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            
            ensemble_results[model_name] = {
                'model': model,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'composite_score': composite_score
            }
            
            logger.info(f"âœ… {model_name} ì™„ë£Œ:")
            logger.info(f"   CV F1: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
            logger.info(f"   Test F1: {f1:.4f}")
            logger.info(f"   Composite: {composite_score:.4f}")
        
        # 8. ìµœê³  ì•™ìƒë¸” ëª¨ë¸ ì„ íƒ
        best_ensemble_name = max(ensemble_results.keys(), 
                                key=lambda x: ensemble_results[x]['composite_score'])
        best_ensemble_info = ensemble_results[best_ensemble_name]
        
        logger.info(f"ğŸ† ìµœê³  ì•™ìƒë¸” ëª¨ë¸: {best_ensemble_name}")
        logger.info(f"   ë³µí•© ì ìˆ˜: {best_ensemble_info['composite_score']:.4f}")
        
        # 9. ì•™ìƒë¸” íš¨ê³¼ ë¶„ì„
        logger.info("ğŸ“Š ì•™ìƒë¸” íš¨ê³¼ ë¶„ì„")
        
        # ê¸°ë³¸ ëª¨ë¸ë“¤ê³¼ ë¹„êµ
        base_model_scores = []
        for name, _ in base_models:
            base_model_scores.append(ensemble_results.get(name, {}).get('composite_score', 0))
        
        avg_base_score = np.mean(base_model_scores)
        best_ensemble_score = best_ensemble_info['composite_score']
        improvement = best_ensemble_score - avg_base_score
        
        logger.info(f"   í‰ê·  ê¸°ë³¸ ëª¨ë¸ ì ìˆ˜: {avg_base_score:.4f}")
        logger.info(f"   ìµœê³  ì•™ìƒë¸” ì ìˆ˜: {best_ensemble_score:.4f}")
        logger.info(f"   ê°œì„ ë„: {improvement:.4f} ({improvement/avg_base_score*100:.1f}%)")
        
        # 10. ê²°ê³¼ ì €ì¥
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)
        
        # ì•™ìƒë¸” ê²°ê³¼ ì €ì¥
        ensemble_path = output_dir / "ensemble_results.pkl"
        joblib.dump(ensemble_results, ensemble_path)
        
        # ìµœê³  ì•™ìƒë¸” ëª¨ë¸ ì €ì¥
        best_ensemble_path = output_dir / "best_ensemble_model.pkl"
        joblib.dump(best_ensemble_info['model'], best_ensemble_path)
        
        # ì„±ëŠ¥ ë¹„êµ CSV
        performance_df = pd.DataFrame({
            'model': list(ensemble_results.keys()),
            'cv_f1_mean': [ensemble_results[m]['cv_mean'] for m in ensemble_results.keys()],
            'cv_f1_std': [ensemble_results[m]['cv_std'] for m in ensemble_results.keys()],
            'test_accuracy': [ensemble_results[m]['accuracy'] for m in ensemble_results.keys()],
            'test_precision': [ensemble_results[m]['precision'] for m in ensemble_results.keys()],
            'test_recall': [ensemble_results[m]['recall'] for m in ensemble_results.keys()],
            'test_f1': [ensemble_results[m]['f1'] for m in ensemble_results.keys()],
            'test_auc': [ensemble_results[m]['auc'] for m in ensemble_results.keys()],
            'composite_score': [ensemble_results[m]['composite_score'] for m in ensemble_results.keys()]
        })
        performance_df = performance_df.sort_values('composite_score', ascending=False)
        performance_df.to_csv(output_dir / "ensemble_model_performance.csv", index=False)
        
        # 11. ê²°ê³¼ ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
        print("="*80)
        print("ğŸ† ìµœê³  ì•™ìƒë¸” ëª¨ë¸:", best_ensemble_name)
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   CV F1:     {best_ensemble_info['cv_mean']:.4f} (Â±{best_ensemble_info['cv_std']:.4f})")
        print(f"   Test Accuracy:  {best_ensemble_info['accuracy']:.4f}")
        print(f"   Test Precision: {best_ensemble_info['precision']:.4f}")
        print(f"   Test Recall:    {best_ensemble_info['recall']:.4f}")
        print(f"   Test F1:        {best_ensemble_info['f1']:.4f}")
        print(f"   Test AUC:       {best_ensemble_info['auc']:.4f}")
        print(f"   Composite:      {best_ensemble_info['composite_score']:.4f}")
        print(f"\nğŸ“ˆ ì•™ìƒë¸” íš¨ê³¼:")
        print(f"   í‰ê·  ê¸°ë³¸ ëª¨ë¸: {avg_base_score:.4f}")
        print(f"   ìµœê³  ì•™ìƒë¸”:    {best_ensemble_score:.4f}")
        print(f"   ê°œì„ ë„:         {improvement:.4f} ({improvement/avg_base_score*100:.1f}%)")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - outputs/ensemble_results.pkl")
        print("  - outputs/best_ensemble_model.pkl")
        print("  - outputs/ensemble_model_performance.csv")
        print("="*80)
        
        # 8. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì¢‹ìœ¼ë©´ ìµœì¢… ëª¨ë¸ ì—…ë°ì´íŠ¸
        current_best_score = max(model_results['model_scores'].values()) if 'model_scores' in model_results else 0
        if 'tuning_improvement' in model_results:
            current_best_score += model_results['tuning_improvement']  # íŠœë‹ ê°œì„ ë¶„ ë°˜ì˜
        if 'regularization_improvement' in model_results:
            current_best_score += model_results['regularization_improvement']  # ì •ê·œí™” ê°œì„ ë¶„ ë°˜ì˜
            
        ensemble_best_score = best_ensemble_info['composite_score']
        
        if ensemble_best_score > current_best_score:
            logger.info(f"ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤! {current_best_score:.4f} â†’ {ensemble_best_score:.4f}")
            
            # ìµœì¢… model_results ì—…ë°ì´íŠ¸
            model_results['best_model'] = best_ensemble_info['model']
            model_results['best_model_name'] = f"{best_ensemble_name} (Ensemble)"
            model_results['ensemble_improvement'] = ensemble_best_score - current_best_score
            
            # ìµœì¢… ëª¨ë¸ ì €ì¥ (outputs/ ë®ì–´ì“°ê¸°)
            outputs_dir = Path(config.output_dir)
            joblib.dump(best_ensemble_info['model'], outputs_dir / "model.pkl")
            joblib.dump(model_results, outputs_dir / "model_results.pkl")
            
            logger.info("âœ… ìµœì¢… ëª¨ë¸ì´ ì•™ìƒë¸” ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤")
            logger.info("ğŸ“Š SHAP ë¶„ì„ì€ 5ë‹¨ê³„(ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸)ì—ì„œ ì¬ìƒì„±ë©ë‹ˆë‹¤")
            
            # ëª¨ë¸ ì„±ëŠ¥ ì •ë³´ ì—…ë°ì´íŠ¸
            from scripts.save_model_performance import save_model_performance
            save_model_performance(model_results)
        else:
            logger.info(f"ê¸°ì¡´ ëª¨ë¸ì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤. {current_best_score:.4f} > {ensemble_best_score:.4f}")
        
        logger.info("âœ… ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    ensemble_modeling()

if __name__ == "__main__":
    main()
