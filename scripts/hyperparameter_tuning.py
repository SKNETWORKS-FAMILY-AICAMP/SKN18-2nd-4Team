#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ì„±ëŠ¥ ìƒìœ„ 3ê°œ ëª¨ë¸ ìµœì í™”: Logistic Regression, SVM, LightGBM
- GridSearchCVë¥¼ í†µí•œ ì²´ê³„ì  íƒìƒ‰
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import joblib

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import make_scorer, f1_score, roc_auc_score


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# LightGBM import
try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
    _has_lgbm = True
except ImportError:
    _has_lgbm = False

def hyperparameter_tuning():
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
    logger.info("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        from src.data.data_loader_new import DataLoaderNew
        from src.utils.config import Config
        
        config = Config("config_final.yaml")
        data_loader = DataLoaderNew(config)
        train_df, test_df = data_loader.load_all_data()
        
        print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
        print(f"  - Train: {train_df.shape[0]:,} rows")
        print(f"  - Test: {test_df.shape[0]:,} rows")
        
        # 2. í”¼ì²˜ ì¤€ë¹„
        target_col = config.target_column
        # 2. ê¸°ë³¸ ëª¨ë¸ë§ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        from src.models.football_modeling import FootballModelTrainer
        
        # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸° (ëª¨ë¸ë§ìš©)
        all_data = pd.concat([train_df, test_df], ignore_index=True)
        
        # ê¸°ë³¸ ëª¨ë¸ë§ ê²°ê³¼ ì¬ì‚¬ìš© (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)
        outputs_dir = Path(config.output_dir)
        model_results_path = outputs_dir / "model_results.pkl"
        
        if model_results_path.exists():
            logger.info("ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ë§ ê²°ê³¼ ì¬ì‚¬ìš© (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)")
            model_results = joblib.load(model_results_path)
        else:
            logger.info("ğŸš€ ê¸°ë³¸ ëª¨ë¸ë§ ê²°ê³¼ê°€ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤")
            model_trainer = FootballModelTrainer(all_data, config)
            model_results = model_trainer.run_pipeline()
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        X_val = model_results['X_val']  # ì „ì²˜ë¦¬ëœ ë°ì´í„°
        y_val = model_results['y_val']
        X_train = model_results['X_train']
        y_train = model_results['y_train']
        preprocessor = model_results['preprocessor']
        
        # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ëª…í™•í•œ ë³€ìˆ˜ëª…ìœ¼ë¡œ í• ë‹¹
        X_train_processed = X_train  # ì „ì²˜ë¦¬ ì™„ë£Œëœ train ë°ì´í„°
        X_val_processed = X_val      # ì „ì²˜ë¦¬ ì™„ë£Œëœ validation ë°ì´í„°
        
        # 5. êµì°¨ ê²€ì¦ ì„¤ì •
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        # 6. í‰ê°€ ì§€í‘œ ì„¤ì • - ë³µí•©ì ìˆ˜ì™€ ì¼ì¹˜ì‹œí‚´
        from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score
        
        def composite_scorer(y_true, y_pred):
            """ë³µí•©ì ìˆ˜ ê³„ì‚°: Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê·  (Precision 40% + F1 30% + Accuracy 20% + Recall 10%)"""
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            # Precisionì„ ì¤‘ì‹œí•˜ëŠ” ê°€ì¤‘í‰ê· 
            composite = (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            return composite
        
        composite_scorer_func = make_scorer(composite_scorer)
        f1_scorer = make_scorer(f1_score)
        auc_scorer = make_scorer(roc_auc_score)
        
        # 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ (ì„±ëŠ¥ ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ)
        param_grids = {}
        
        # 2ë‹¨ê³„: ìµœì  êµ¬ê°„ ì£¼ë³€ ì„¸ë°€ íƒìƒ‰ (3ê°œ ëª¨ë¸ ëª¨ë‘)
        # Logistic Regression - 1.0 ì£¼ë³€ ì„¸ë°€ íƒìƒ‰
        param_grids['Logistic Regression'] = {
            'C': [0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3],  # 1.0 ì£¼ë³€ ì„¸ë°€í•˜ê²Œ
            'penalty': ['l2'],  # ì„±ê³µí•œ penalty ìœ ì§€
            'max_iter': [1000]
        }
        
        # SVM - 0.1 ì£¼ë³€ ì„¸ë°€ íƒìƒ‰ (linear kernel)
        param_grids['SVM'] = {
            'C': [0.05, 0.08, 0.1, 0.12, 0.15, 0.2],  # 0.1 ì£¼ë³€ ì„¸ë°€í•˜ê²Œ
            'kernel': ['linear'],  # ì„±ê³µí•œ kernel ìœ ì§€
            'gamma': ['scale']
        }
        
        # LightGBM - ê¸°ë³¸ê°’ ì£¼ë³€ ì„¸ë°€ íƒìƒ‰ (íŠœë‹ ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ê¸°ì¤€)
        if _has_lgbm:
            param_grids['LightGBM'] = {
                'n_estimators': [50, 100, 150, 200, 250],  # ê¸°ë³¸ê°’ 100 ì£¼ë³€
                'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25],  # ê¸°ë³¸ê°’ 0.1 ì£¼ë³€
                'max_depth': [3, 4, 5, 6, 7]  # ê¸°ë³¸ê°’ 3 ì£¼ë³€
            }
        
        # 8. ëª¨ë¸ ì •ì˜ (ì„±ëŠ¥ ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ)
        from sklearn.linear_model import LogisticRegression
        from sklearn.svm import SVC
        
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),
            'SVM': SVC(random_state=42, class_weight='balanced', probability=True)
        }
        
        if _has_lgbm:
            models['LightGBM'] = LGBMClassifier(random_state=42, class_weight='balanced')
        
        # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œì—ì„œ classifier__ ì ‘ë‘ì‚¬ ì œê±° (Pipeline ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        for model_name in param_grids:
            new_params = {}
            for param, values in param_grids[model_name].items():
                # classifier__ ì ‘ë‘ì‚¬ ì œê±°
                new_param = param.replace('classifier__', '')
                new_params[new_param] = values
            param_grids[model_name] = new_params
        
        # 9. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ì „ì²˜ë¦¬ëœ ë°ì´í„°ì— ì§ì ‘ ì ìš©)
        tuning_results = {}
        
        for model_name, model in models.items():
            if model_name not in param_grids:
                continue
                
            logger.info(f"ğŸ”§ {model_name} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
            
            # íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± ì²˜ë¦¬
            current_params = param_grids[model_name].copy()
            
            # Logistic Regressionì˜ penalty-solver í˜¸í™˜ì„± ì²˜ë¦¬
            if model_name == 'Logistic Regression':
                # penaltyì— ë”°ë¼ ì ì ˆí•œ solver ìë™ ì„ íƒ
                compatible_params = []
                for penalty in current_params['penalty']:
                    param_combo = current_params.copy()
                    param_combo['penalty'] = [penalty]
                    
                    # penaltyì— ë”°ë¥¸ solver ìë™ ì„ íƒ
                    if penalty == 'l1':
                        param_combo['solver'] = ['liblinear']  # l1ì€ liblinear ì‚¬ìš©
                        param_combo.pop('l1_ratio', None)     # l1ì€ l1_ratio ë¶ˆí•„ìš”
                    elif penalty == 'l2':
                        param_combo['solver'] = ['lbfgs']      # l2ëŠ” lbfgs ì‚¬ìš© (ë¹ ë¦„)
                        param_combo.pop('l1_ratio', None)     # l2ëŠ” l1_ratio ë¶ˆí•„ìš”
                    elif penalty == 'elasticnet':
                        param_combo['solver'] = ['saga']       # elasticnetì€ saga ì‚¬ìš©
                        # l1_ratioëŠ” ìœ ì§€ (elasticnet í•„ìˆ˜ íŒŒë¼ë¯¸í„°)
                    
                    compatible_params.append(param_combo)
                
                # ê° í˜¸í™˜ ì¡°í•©ë³„ë¡œ ê°œë³„ íŠœë‹
                best_score = -1
                best_params = None
                best_estimator = None
                
                for param_combo in compatible_params:
                    grid_search = GridSearchCV(
                        model,
                        param_combo,
                        cv=cv,
                        scoring=composite_scorer_func,
                        n_jobs=-1,
                        verbose=0
                    )
                    grid_search.fit(X_train_processed, y_train)
                    
                    if grid_search.best_score_ > best_score:
                        best_score = grid_search.best_score_
                        best_params = grid_search.best_params_
                        best_estimator = grid_search.best_estimator_
                
                # ê²°ê³¼ ì €ì¥
                tuning_results[model_name] = {
                    'best_params': best_params,
                    'best_score': best_score,
                    'best_model': best_estimator
                }
                
            else:
                # SVMê³¼ LightGBMì€ ê¸°ì¡´ ë°©ì‹
                grid_search = GridSearchCV(
                    model,
                    current_params,
                    cv=cv,
                    scoring=composite_scorer_func,
                    n_jobs=-1,
                    verbose=1
                )
                
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
                grid_search.fit(X_train_processed, y_train)
                
                # ê²°ê³¼ ì €ì¥
                tuning_results[model_name] = {
                    'best_params': grid_search.best_params_,
                    'best_score': grid_search.best_score_,
                    'best_model': grid_search.best_estimator_
                }
            
            logger.info(f"âœ… {model_name} íŠœë‹ ì™„ë£Œ")
            logger.info(f"   ìµœê³  ì ìˆ˜: {tuning_results[model_name]['best_score']:.4f}")
            logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {tuning_results[model_name]['best_params']}")
        
        # 10. ìµœì  ëª¨ë¸ í‰ê°€
        logger.info("ğŸ“Š ìµœì  ëª¨ë¸ í‰ê°€ ì‹œì‘")
        
        best_models = {}
        for model_name, results in tuning_results.items():
            best_model = results['best_model']
            
            # ì˜ˆì¸¡
            y_pred = best_model.predict(X_val_processed)
            y_pred_proba = best_model.predict_proba(X_val_processed)[:, 1]
            
            # ì„±ëŠ¥ í‰ê°€
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred)
            recall = recall_score(y_val, y_pred)
            f1 = f1_score(y_val, y_pred)
            auc = roc_auc_score(y_val, y_pred_proba)
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (ê· ë“± ê°€ì¤‘)
            # Precision ì¤‘ì‹¬ ê°€ì¤‘í‰ê·  (GridSearchCVì™€ ì¼ì¹˜)
            composite_score = (precision * 0.4 + f1 * 0.3 + accuracy * 0.2 + recall * 0.1)
            
            best_models[model_name] = {
                'model': best_model,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'composite_score': composite_score,
                'best_params': results['best_params']
            }
            
            logger.info(f"âœ… {model_name} í‰ê°€ ì™„ë£Œ:")
            logger.info(f"   Accuracy: {accuracy:.4f}")
            logger.info(f"   Precision: {precision:.4f}")
            logger.info(f"   Recall: {recall:.4f}")
            logger.info(f"   F1-Score: {f1:.4f}")
            logger.info(f"   AUC: {auc:.4f}")
            logger.info(f"   Composite: {composite_score:.4f}")
        
        # 11. ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model_name = max(best_models.keys(), key=lambda x: best_models[x]['composite_score'])
        best_model_info = best_models[best_model_name]
        
        logger.info(f"ğŸ† ìµœê³  ëª¨ë¸: {best_model_name}")
        logger.info(f"   ë³µí•© ì ìˆ˜: {best_model_info['composite_score']:.4f}")
        
        # 12. ê²°ê³¼ ì €ì¥
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)
        
        # íŠœë‹ ê²°ê³¼ ì €ì¥
        tuning_results_path = output_dir / "hyperparameter_tuning_results.pkl"
        joblib.dump(tuning_results, tuning_results_path)
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        best_model_path = output_dir / "best_tuned_model.pkl"
        joblib.dump(best_model_info['model'], best_model_path)
        
        # ì„±ëŠ¥ ë¹„êµ CSV
        performance_df = pd.DataFrame({
            'model': list(best_models.keys()),
            'accuracy': [best_models[m]['accuracy'] for m in best_models.keys()],
            'precision': [best_models[m]['precision'] for m in best_models.keys()],
            'recall': [best_models[m]['recall'] for m in best_models.keys()],
            'f1_score': [best_models[m]['f1'] for m in best_models.keys()],
            'auc': [best_models[m]['auc'] for m in best_models.keys()],
            'composite_score': [best_models[m]['composite_score'] for m in best_models.keys()]
        })
        performance_df = performance_df.sort_values('composite_score', ascending=False)
        performance_df.to_csv(output_dir / "tuned_model_performance.csv", index=False)
        
        # 13. ê²°ê³¼ ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
        print("="*80)
        print("ğŸ† ìµœê³  ëª¨ë¸:", best_model_name)
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   Accuracy:  {best_model_info['accuracy']:.4f}")
        print(f"   Precision: {best_model_info['precision']:.4f}")
        print(f"   Recall:    {best_model_info['recall']:.4f}")
        print(f"   F1-Score:  {best_model_info['f1']:.4f}")
        print(f"   AUC:       {best_model_info['auc']:.4f}")
        print(f"   Composite: {best_model_info['composite_score']:.4f}")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - outputs/hyperparameter_tuning_results.pkl")
        print("  - outputs/best_tuned_model.pkl")
        print("  - outputs/tuned_model_performance.csv")
        print("="*80)
        
        # 8. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì¢‹ìœ¼ë©´ ìµœì¢… ëª¨ë¸ ì—…ë°ì´íŠ¸
        original_best_score = max(model_results['model_scores'].values()) if 'model_scores' in model_results else 0
        tuned_best_score = best_model_info['composite_score']
        
        if tuned_best_score > original_best_score:
            logger.info(f"ğŸ‰ íŠœë‹ëœ ëª¨ë¸ì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤! {original_best_score:.4f} â†’ {tuned_best_score:.4f}")
            
            # ìµœì¢… model_results ì—…ë°ì´íŠ¸
            model_results['best_model'] = best_model_info['model']
            model_results['best_model_name'] = f"{best_model_name} (Tuned)"
            model_results['tuning_improvement'] = tuned_best_score - original_best_score
            
            # ìµœì¢… ëª¨ë¸ ì €ì¥ (outputs/ ë®ì–´ì“°ê¸°)
            outputs_dir = Path(config.output_dir)
            joblib.dump(best_model_info['model'], outputs_dir / "model.pkl")
            joblib.dump(model_results, outputs_dir / "model_results.pkl")
            
            logger.info("âœ… ìµœì¢… ëª¨ë¸ì´ íŠœë‹ëœ ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            # ëª¨ë¸ ì„±ëŠ¥ ì •ë³´ ì—…ë°ì´íŠ¸
            from scripts.save_model_performance import save_model_performance
            save_model_performance(model_results)
        else:
            logger.info(f"ê¸°ì¡´ ëª¨ë¸ì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤. {original_best_score:.4f} > {tuned_best_score:.4f}")
        
        logger.info("âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    hyperparameter_tuning()

if __name__ == "__main__":
    main()
