#!/usr/bin/env python3
"""
ëª¨ë¸ ì„±ëŠ¥ ì ìˆ˜ë¥¼ ìƒì„¸íˆ ì €ì¥í•˜ê³  ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import joblib
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def save_model_performance():
    """ëª¨ë¸ ì„±ëŠ¥ ì ìˆ˜ë¥¼ ìƒì„¸íˆ ì €ì¥í•˜ê³  ì¶œë ¥"""
    try:
        # ê²½ë¡œ ì„¤ì •
        outputs_dir = Path("outputs")
        model_results_path = outputs_dir / "model_results.pkl"
        
        if not model_results_path.exists():
            logger.error("model_results.pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëª¨ë¸ ê²°ê³¼ ë¡œë“œ
        model_results = joblib.load(model_results_path)
        
        if 'model_comparison' not in model_results:
            logger.error("ëª¨ë¸ ë¹„êµ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        model_comparison = model_results['model_comparison']
        
        # ìƒì„¸ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        detailed_performance = []
        
        for model_name, metrics in model_comparison.items():
            # metricsê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if isinstance(metrics, (int, float)):
                # ë‹¨ìˆœ ì ìˆ˜ì¸ ê²½ìš° Composite_Scoreë¡œ ì²˜ë¦¬
                detailed_performance.append({
                    'Model': model_name,
                    'Accuracy': 0,
                    'Precision': 0,
                    'Recall': 0,
                    'F1_Score': 0,
                    'AUC': 0,
                    'Composite_Score': round(metrics, 4)
                })
            else:
                # dictì¸ ê²½ìš° ì •ìƒ ì²˜ë¦¬
                detailed_performance.append({
                    'Model': model_name,
                    'Accuracy': round(metrics.get('Accuracy', 0), 4),
                    'Precision': round(metrics.get('Precision', 0), 4),
                    'Recall': round(metrics.get('Recall', 0), 4),
                    'F1_Score': round(metrics.get('F1_Score', 0), 4),
                    'AUC': round(metrics.get('AUC', 0), 4),
                    'Composite_Score': round(metrics.get('Composite_Score', 0), 4)
                })
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬
        df_performance = pd.DataFrame(detailed_performance)
        df_performance = df_performance.sort_values('Composite_Score', ascending=False)
        df_performance['Rank'] = range(1, len(df_performance) + 1)
        
        # ìˆœìœ„ë³„ ì´ëª¨ì§€ ì¶”ê°€
        rank_emoji = {1: 'ğŸ¥‡', 2: 'ğŸ¥ˆ', 3: 'ğŸ¥‰'}
        df_performance['Medal'] = df_performance['Rank'].map(lambda x: rank_emoji.get(x, ''))
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        df_performance = df_performance[['Rank', 'Medal', 'Model', 'Composite_Score', 
                                       'Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC']]
        
        # CSV ì €ì¥
        performance_csv_path = outputs_dir / "detailed_model_performance.csv"
        df_performance.to_csv(performance_csv_path, index=False, encoding='utf-8-sig')
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š **ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ê²°ê³¼**")
        print("="*80)
        
        for idx, row in df_performance.iterrows():
            print(f"\n{row['Medal']} **{row['Rank']}ìœ„: {row['Model']}**")
            print(f"   â€¢ ì¢…í•©ì ìˆ˜: {row['Composite_Score']:.4f}")
            print(f"   â€¢ ì •í™•ë„  : {row['Accuracy']:.4f}")
            print(f"   â€¢ ì •ë°€ë„  : {row['Precision']:.4f}")
            print(f"   â€¢ ì¬í˜„ìœ¨  : {row['Recall']:.4f}")
            print(f"   â€¢ F1ì ìˆ˜  : {row['F1_Score']:.4f}")
            print(f"   â€¢ AUC     : {row['AUC']:.4f}")
        
        print("\n" + "="*80)
        print(f"âœ… ìƒì„¸ ì„±ëŠ¥ ê²°ê³¼ê°€ {performance_csv_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì±„íƒ ëª¨ë¸ ì •ë³´
        best_model_info = {
            'Best_Model': df_performance.iloc[0]['Model'],
            'Best_Score': df_performance.iloc[0]['Composite_Score'],
            'Selection_Criteria': 'Composite Score (Weighted Average)',
            'Weight_Formula': 'Accuracy(0.2) + Precision(0.2) + Recall(0.2) + F1(0.2) + AUC(0.2)'
        }
        
        # ì±„íƒ ëª¨ë¸ ì •ë³´ CSV ì €ì¥
        adoption_csv_path = outputs_dir / "model_adoption_info.csv"
        pd.DataFrame([best_model_info]).to_csv(adoption_csv_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"âœ… ëª¨ë¸ ì„±ëŠ¥ ì €ì¥ ì™„ë£Œ: {len(df_performance)}ê°œ ëª¨ë¸")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì„±ëŠ¥ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    save_model_performance()

if __name__ == "__main__":
    main()